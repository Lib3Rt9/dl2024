{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "import math\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 7, Validation set size: 1, Test set size: 2\n",
      "Unique classes in the training set: [0, 1, 2, 3, 4]\n",
      "Number of classes for prediction: 7\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        image_files = os.listdir(self.directory)\n",
    "        for img in image_files[:10]:\n",
    "            try:\n",
    "                img_path = os.path.join(self.directory, img)\n",
    "                with Image.open(img_path) as img_array:\n",
    "                    gray_array = img_array.convert('L')\n",
    "                    resized_array = gray_array.resize((150, 150))\n",
    "                    self.images.append(list(resized_array.getdata()))\n",
    "                    label = img.split('_')[0]\n",
    "                    self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        max_pixel_value = 255.0\n",
    "        self.images = [[pixel / max_pixel_value for pixel in image] for image in self.images]\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        label_to_int = {label: index for index, label in enumerate(unique_labels)}\n",
    "        self.label_indices = [label_to_int[label] for label in self.labels]\n",
    "        self.labels = [[int(i == label_index) for i in range(len(unique_labels))] for label_index in self.label_indices]\n",
    "\n",
    "def split_data(images, labels, train_ratio, validation_ratio, test_ratio):\n",
    "    combined = list(zip(images, labels))\n",
    "    shuffle(combined)\n",
    "    shuffled_images, shuffled_labels = zip(*combined)\n",
    "\n",
    "    train_end = int(len(shuffled_images) * train_ratio)\n",
    "    validation_end = train_end + int(len(shuffled_images) * validation_ratio)\n",
    "\n",
    "    x_train = shuffled_images[:train_end]\n",
    "    y_train = shuffled_labels[:train_end]\n",
    "    x_val = shuffled_images[train_end:validation_end]\n",
    "    y_val = shuffled_labels[train_end:validation_end]\n",
    "    x_test = shuffled_images[validation_end:]\n",
    "    y_test = shuffled_labels[validation_end:]\n",
    "\n",
    "    return list(x_train), list(x_val), list(x_test), list(y_train), list(y_val), list(y_test)\n",
    "\n",
    "# Usage\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "dataset.one_hot_encode()\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(dataset.images, dataset.labels, 0.75, 0.15, 0.10)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = sorted(set([label.index(1) for label in y_train]))\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")\n",
    "\n",
    "num_classes = len(set(dataset.label_indices))\n",
    "print(f\"Number of classes for prediction: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer class\n",
    "class Layer:\n",
    "    def forward(self, input_data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Convolutional Layer\n",
    "class ConvLayer(Layer):\n",
    "    def __init__(self, num_filters, filter_size, padding=0, stride=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) / 9  # normalize values\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "    \n",
    "        # Get the width, height and depth of the input\n",
    "        h, w, d = input_data.shape\n",
    "\n",
    "        # Print the input dimensions, filter size, padding, and stride\n",
    "        print(f\"Input dimensions: height={h}, width={w}, depth={d}\")\n",
    "        print(f\"Filter size: {self.filter_size}\")\n",
    "        print(f\"Padding: {self.padding}, Stride: {self.stride}\")\n",
    "        \n",
    "        # Apply padding to the input\n",
    "        input_padded = np.pad(input_data, ((self.padding, self.padding), (self.padding, self.padding), (0, 0)), 'constant')\n",
    "        \n",
    "        # Update the output dimensions calculation to include padding and stride\n",
    "        new_h = int((h + 2 * self.padding - self.filter_size) / self.stride) + 1\n",
    "        new_w = int((w + 2 * self.padding - self.filter_size) / self.stride) + 1\n",
    "\n",
    "        # Print the calculated output dimensions\n",
    "        print(f\"Calculated output dimensions: new_height={new_h}, new_width={new_w}\")\n",
    "        \n",
    "        output = np.zeros((new_h, new_w, self.num_filters))\n",
    "        \n",
    "        # Perform the convolution operation\n",
    "        for i in range(h - self.filter_size + 1):\n",
    "            for j in range(w - self.filter_size + 1):\n",
    "                for f in range(self.num_filters):\n",
    "                    # Apply the filters to the input\n",
    "                    output[i, j, f] = np.sum(input_data[i:i+self.filter_size, j:j+self.filter_size] * self.filters[f])\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        d_filters = np.zeros(self.filters.shape)\n",
    "        d_input = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        h, w, d = output_error.shape\n",
    "        h_i, w_i, d_i = self.last_input.shape\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                for f in range(self.num_filters):\n",
    "                    # Update the gradients for the filters\n",
    "                    d_filters[f] += output_error[i, j, f] * self.last_input[i:i+self.filter_size, j:j+self.filter_size]\n",
    "                    \n",
    "                    # Calculate the gradient with respect to the input\n",
    "                    d_input[i:i+self.filter_size, j:j+self.filter_size] += self.filters[f] * output_error[i, j, f]\n",
    "        \n",
    "        # Update the filters\n",
    "        self.filters -= learning_rate * d_filters\n",
    "        \n",
    "        return d_input\n",
    "        \n",
    "# MaxPooling Layer\n",
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        h, w, num_filters = input_data.shape\n",
    "        output = np.zeros((h // self.pool_size, w // self.pool_size, num_filters))\n",
    "        for i in range(h // self.pool_size):\n",
    "            for j in range(w // self.pool_size):\n",
    "                for f in range(num_filters):\n",
    "                    output[i, j, f] = np.max(input_data[i*self.pool_size:(i+1)*self.pool_size, j*self.pool_size:(j+1)*self.pool_size, f])\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        d_input = np.zeros(self.last_input.shape)\n",
    "        h, w, num_filters = self.last_input.shape\n",
    "        for i in range(h // self.pool_size):\n",
    "            for j in range(w // self.pool_size):\n",
    "                for f in range(num_filters):\n",
    "                    h_start, w_start = i*self.pool_size, j*self.pool_size\n",
    "                    patch = self.last_input[h_start:h_start+self.pool_size, w_start:w_start+self.pool_size, f]\n",
    "                    h_i, w_i = np.unravel_index(np.argmax(patch), patch.shape)\n",
    "                    d_input[h_start+h_i, w_start+w_i, f] = output_error[i, j, f]\n",
    "        return d_input\n",
    "        \n",
    "class FlattenLayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        self.last_input_shape = input_data.shape\n",
    "        return input_data.flatten()\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return output_error.reshape(self.last_input_shape)\n",
    "        \n",
    "# Dropout Layer\n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.mask = (np.random.rand(*input_data.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "        return input_data * self.mask\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return output_error * self.mask\n",
    "\n",
    "# Dense Layer\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        d_weights = np.outer(self.last_input, output_error)\n",
    "        d_biases = output_error\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.biases -= learning_rate * d_biases\n",
    "        return np.dot(output_error, self.weights.T)\n",
    "        \n",
    "class SoftmaxLayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        exp = np.exp(input_data - np.max(input_data, axis=0))  # subtract max for numerical stability\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return output_error  # Error is passed straight through to the next layer\n",
    "        \n",
    "# ReLU Activation\n",
    "class ReLULayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # The derivative of ReLU is 1 for positive inputs and 0 for negative inputs\n",
    "        return output_error * (self.last_input > 0)\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "class CrossEntropyLoss:\n",
    "    def calculate_loss(self, predicted, actual):\n",
    "        return -np.sum(actual * np.log(predicted + 1e-9))  # add small constant to avoid log(0)\n",
    "        \n",
    "    def calculate_gradient(self, predicted, actual):\n",
    "        return predicted - actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: height=22500, width=1, depth=1\n",
      "Filter size: 3\n",
      "Padding: 0, Stride: 1\n",
      "Calculated output dimensions: new_height=22498, new_width=-1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(predictions)\n\u001b[1;32m     91\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m---> 92\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m, in \u001b[0;36mCNN.train\u001b[0;34m(self, X_train, y_train, X_val, y_val, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train, X_val, y_val, epochs, learning_rate):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Calculate the output size after the convolution and pooling layers\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     sample_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     flattened_size \u001b[38;5;241m=\u001b[39m sample_output\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Update the DenseLayer with the correct input size\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     21\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mConvLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Print the calculated output dimensions\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated output dimensions: new_height=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, new_width=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Perform the convolution operation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "class CNN:\n",
    "    def __init__(self, threshold=0.5, early_stopping_patience=5):\n",
    "        self.layers = [\n",
    "            ConvLayer(num_filters=32, filter_size=3),\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            FlattenLayer(),\n",
    "            DenseLayer(input_size=32*74*74, output_size=num_classes),  # Assuming the input image size is 150x150\n",
    "            SoftmaxLayer()\n",
    "        ]\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.threshold = threshold\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_loss = np.inf\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            # Add a channel dimension to X if it's missing\n",
    "            if len(X.shape) == 2:\n",
    "                X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        output_error = self.loss.calculate_gradient(X, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        return self.loss.calculate_loss(X, y)\n",
    "\n",
    "    def calculate_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate):\n",
    "        # Calculate the output size after the convolution and pooling layers\n",
    "        sample_output = self.forward(np.expand_dims(X_train[0], axis=-1))\n",
    "        flattened_size = sample_output.size\n",
    "        \n",
    "        # Update the DenseLayer with the correct input size\n",
    "        self.layers[-2] = DenseLayer(input_size=flattened_size, output_size=num_classes)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "            \n",
    "            for j in range(len(X_train)):\n",
    "                # Ensure the input data has three dimensions: height, width, and depth\n",
    "                input_data = np.expand_dims(X_train[j], axis=-1)  # Add the depth dimension\n",
    "                output = self.forward(input_data)\n",
    "                self.backward(output, y_train[j], learning_rate)\n",
    "                \n",
    "                # Calculate loss and accuracy for each training example\n",
    "                train_loss = self.calculate_loss(output, y_train[j])\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = self.calculate_accuracy(output, y_train[j])\n",
    "                train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Calculate average training loss and accuracy\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            avg_train_accuracy = np.mean(train_accuracies)\n",
    "            \n",
    "            # Validation loss and accuracy\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = self.calculate_loss(val_output, y_val)\n",
    "            val_accuracy = self.calculate_accuracy(val_output, y_val)\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss}, Train Accuracy: {avg_train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            if self.patience_counter >= self.early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            output = self.forward(X[i])\n",
    "            predictions.append(np.argmax(output))\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "\n",
    "cnn = CNN()\n",
    "cnn.train(x_train, y_train, x_val, y_val, epochs=5, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfe682-9d17-4aea-afdb-14218be9633c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337c04-c9f5-413a-87b8-72d93bdfe88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
