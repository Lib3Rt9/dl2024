{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 20\n",
      "Dimensions of each image: (150, 150, 3)\n",
      "Total dimensions of the dataset (assuming all images have the same shape): 20 x (150, 150, 3)\n",
      "Train set size: 15, Validation set size: 3, Test set size: 2\n",
      "Unique classes in the training set: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Number of classes for prediction: 10\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        image_files = os.listdir(self.directory)\n",
    "        for img in image_files[:20]:\n",
    "            try:\n",
    "                img_path = os.path.join(self.directory, img)\n",
    "                with Image.open(img_path) as img_array:\n",
    "                    resized_array = img_array.resize((150, 150))\n",
    "                    self.images.append(np.array(resized_array))\n",
    "                    label = img.split('_')[0]\n",
    "                    self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        max_pixel_value = 255.0\n",
    "        self.images = [image / max_pixel_value for image in self.images]\n",
    "\n",
    "    def print_dataset_dimensions(self):\n",
    "        if self.images:\n",
    "            print(f\"Total number of images: {len(self.images)}\")\n",
    "            print(f\"Dimensions of each image: {self.images[0].shape}\")\n",
    "            print(f\"Total dimensions of the dataset (assuming all images have the same shape): {len(self.images)} x {self.images[0].shape}\")\n",
    "        else:\n",
    "            print(\"The dataset is empty or not loaded properly.\")\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        label_to_int = {label: index for index, label in enumerate(unique_labels)}\n",
    "        self.label_indices = [label_to_int[label] for label in self.labels]\n",
    "        self.labels = [[int(i == label_index) for i in range(len(unique_labels))] for label_index in self.label_indices]\n",
    "\n",
    "def split_data(images, labels, train_ratio, validation_ratio, test_ratio):\n",
    "    combined = list(zip(images, labels))\n",
    "    shuffle(combined)\n",
    "    shuffled_images, shuffled_labels = zip(*combined)\n",
    "\n",
    "    train_end = int(len(shuffled_images) * train_ratio)\n",
    "    validation_end = train_end + int(len(shuffled_images) * validation_ratio)\n",
    "\n",
    "    x_train = shuffled_images[:train_end]\n",
    "    y_train = shuffled_labels[:train_end]\n",
    "    x_val = shuffled_images[train_end:validation_end]\n",
    "    y_val = shuffled_labels[train_end:validation_end]\n",
    "    x_test = shuffled_images[validation_end:]\n",
    "    y_test = shuffled_labels[validation_end:]\n",
    "\n",
    "    return list(x_train), list(x_val), list(x_test), list(y_train), list(y_val), list(y_test)\n",
    "\n",
    "# Usage\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "dataset.one_hot_encode()\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(dataset.images, dataset.labels, 0.75, 0.15, 0.10)\n",
    "\n",
    "# Print the dimensions of the preprocessed images\n",
    "dataset.print_dataset_dimensions()\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = sorted(set([label.index(1) for label in y_train]))\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")\n",
    "\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Number of classes for prediction: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9439cb68-4913-46ff-ade6-c02ec7dfe7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Limiting:\n",
    "    @staticmethod\n",
    "    def clip(value, min_value, max_value):\n",
    "        return max(min(value, max_value), min_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def min_max_scale(data):\n",
    "        min_val = min(data)\n",
    "        max_val = max(data)\n",
    "        range_val = max_val - min_val\n",
    "        if range_val == 0:\n",
    "            return [0 for _ in data]  # Return a list of zeros if all values are the same\n",
    "        scaled_data = [(value - min_val) / range_val for value in data]\n",
    "        return scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer class\n",
    "class Layer:\n",
    "    def forward(self, input_data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Convolutional Layer\n",
    "class ConvLayer(Layer):\n",
    "    def __init__(self, num_filters, filter_size, num_channels=3, padding=0, stride=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[[random.uniform(-1, 1) for _ in range(num_channels)] for _ in range(filter_size)] for _ in range(num_filters)]\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def pad_input(self, input_data):\n",
    "        # Initialize the padded data with zeros for the new dimensions\n",
    "        padded_data = [[[0 for _ in range(len(input_data[0][0]) + 2 * self.padding)]\n",
    "                        for _ in range(len(input_data[0]) + 2 * self.padding)]\n",
    "                        for _ in range(len(input_data))]\n",
    "\n",
    "        # Apply padding to each channel of the input data\n",
    "        for z in range(len(input_data)):\n",
    "            for y in range(len(input_data[0])):\n",
    "                for x in range(len(input_data[0][0])):\n",
    "                    padded_data[z][y + self.padding][x + self.padding] = input_data[z][y][x]\n",
    "\n",
    "        return padded_data\n",
    "\n",
    "    def element_wise_multiplication(self, matrix1, matrix2):\n",
    "        # Assuming matrix1 and matrix2 are 2D lists of the same size\n",
    "        return [[matrix1[i][j] * matrix2[i][j] for j in range(len(matrix1[0]))] for i in range(len(matrix1))]\n",
    "\n",
    "    def sum_matrix(self, matrix):\n",
    "        # Sum all elements in a 2D list\n",
    "        return sum(sum(row) for row in matrix)\n",
    "\n",
    "    def clip(self, value, min_value, max_value):\n",
    "        # Clip the value to be within the min and max range\n",
    "        return max(min_value, min(value, max_value))\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # Assuming input_batch is a 4D list: [batch_size, height, width, depth]\n",
    "        batch_size = len(input_batch)\n",
    "        output_batch = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            input_data = input_batch[b]\n",
    "            input_padded = self.pad_input(input_data)\n",
    "\n",
    "            # Calculate output dimensions for each image in the batch\n",
    "            output_height = int((len(input_padded) - self.filter_size) / self.stride) + 1\n",
    "            output_width = int((len(input_padded[0]) - self.filter_size) / self.stride) + 1\n",
    "\n",
    "            # Initialize the output for the current image as a list of lists\n",
    "            output = [[0 for _ in range(output_width)] for _ in range(output_height)]\n",
    "\n",
    "            # Perform the convolution operation for the current image\n",
    "            for i in range(output_height):\n",
    "                for j in range(output_width):\n",
    "                    for f in range(self.num_filters):\n",
    "                        vertical_start = i * self.stride\n",
    "                        vertical_end = vertical_start + self.filter_size\n",
    "                        horizontal_start = j * self.stride\n",
    "                        horizontal_end = horizontal_start + self.filter_size\n",
    "\n",
    "                        # Extract the patch from the padded input\n",
    "                        patch = [row[horizontal_start:horizontal_end] for row in input_padded[vertical_start:vertical_end]]\n",
    "\n",
    "                        # Perform element-wise multiplication and sum the result\n",
    "                        multiplied_patch = self.element_wise_multiplication(patch, self.filters[f])\n",
    "                        conv_sum = self.sum_matrix(multiplied_patch)\n",
    "\n",
    "                        # Clip the values and assign to the output\n",
    "                        output[i][j] = Limiting.clip(conv_sum, -1e2, 1e2)\n",
    "\n",
    "            # Append the output for the current image to the output batch\n",
    "            output_batch.append(output)\n",
    "\n",
    "        return output_batch\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # Initialize gradients with zeros\n",
    "        input_gradient = [[0 for _ in range(len(self.last_input[0]))] for _ in range(len(self.last_input))]\n",
    "        filter_gradients = [[[0 for _ in range(len(self.filters[0][0]))] for _ in range(len(self.filters[0]))] for _ in range(len(self.filters))]\n",
    "\n",
    "        # Loop over the output gradient\n",
    "        for i in range(len(output_gradient)):\n",
    "            for j in range(len(output_gradient[0])):\n",
    "                for f in range(len(self.filters)):\n",
    "                    # Calculate the position in the input\n",
    "                    vertical_start = i * self.stride\n",
    "                    vertical_end = vertical_start + self.filter_size\n",
    "                    horizontal_start = j * self.stride\n",
    "                    horizontal_end = horizontal_start + self.filter_size\n",
    "\n",
    "                    # Extract the patch from the last input\n",
    "                    input_patch = [row[horizontal_start:horizontal_end] for row in self.last_input[vertical_start:vertical_end]]\n",
    "\n",
    "                    # Calculate the filter gradient\n",
    "                    for m in range(self.filter_size):\n",
    "                        for n in range(self.filter_size):\n",
    "                            filter_gradients[f][m][n] += input_patch[m][n] * output_gradient[i][j]\n",
    "\n",
    "                    # Calculate the input gradient\n",
    "                    for m in range(self.filter_size):\n",
    "                        for n in range(self.filter_size):\n",
    "                            input_gradient[vertical_start + m][horizontal_start + n] += self.filters[f][m][n] * output_gradient[i][j]\n",
    "\n",
    "        # Update the filters\n",
    "        for f in range(len(self.filters)):\n",
    "            for m in range(self.filter_size):\n",
    "                for n in range(self.filter_size):\n",
    "                    self.filters[f][m][n] -= learning_rate * filter_gradients[f][m][n]\n",
    "\n",
    "        return input_gradient\n",
    "        \n",
    "# MaxPooling Layer\n",
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        self.last_input = None\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # Assuming input_batch is a 4D list: [batch_size, height, width, depth]\n",
    "        batch_size = len(input_batch)\n",
    "        output_batch = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            input_data = input_batch[b]\n",
    "            # Calculate output dimensions for each image in the batch\n",
    "            output_height = len(input_data) // self.pool_size\n",
    "            output_width = len(input_data[0]) // self.pool_size\n",
    "            output_depth = len(input_data[0][0])\n",
    "\n",
    "            # Initialize the output for the current image as a list of lists\n",
    "            output = [[[0 for _ in range(output_depth)] for _ in range(output_width)] for _ in range(output_height)]\n",
    "\n",
    "            # Perform the max pooling operation for the current image\n",
    "            for z in range(output_depth):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        # Find the maximum value in the current pool\n",
    "                        pool = [input_data[z][i * self.pool_size + di][j * self.pool_size + dj]\n",
    "                                for di in range(self.pool_size) for dj in range(self.pool_size)]\n",
    "                        output[i][j][z] = max(pool)\n",
    "\n",
    "            # Append the output for the current image to the output batch\n",
    "            output_batch.append(output)\n",
    "\n",
    "        return output_batch\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        input_gradient = [[[0 for _ in range(len(self.last_input[0][0]))] for _ in range(len(self.last_input[0]))] for _ in range(len(self.last_input))]\n",
    "\n",
    "        for z in range(len(self.last_input[0][0])):\n",
    "            for i in range(len(output_gradient)):\n",
    "                for j in range(len(output_gradient[0])):\n",
    "                    patch = [self.last_input[i * self.pool_size + di][j * self.pool_size + dj][z]\n",
    "                             for di in range(self.pool_size) for dj in range(self.pool_size)]\n",
    "                    max_value = max(patch)\n",
    "                    for di in range(self.pool_size):\n",
    "                        for dj in range(self.pool_size):\n",
    "                            if self.last_input[i * self.pool_size + di][j * self.pool_size + dj][z] == max_value:\n",
    "                                input_gradient[i * self.pool_size + di][j * self.pool_size + dj][z] = output_gradient[i][j][z]\n",
    "                                break\n",
    "        return input_gradient\n",
    "        \n",
    "class FlattenLayer:\n",
    "    def forward(self, input_data):\n",
    "        # Save the input shape to rebuild it during the backward pass\n",
    "        self.last_input_shape = self.get_shape(input_data)\n",
    "        \n",
    "        # Flatten the input data\n",
    "        flattened_data = self.flatten(input_data)\n",
    "        return flattened_data\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Reshape the output error to the original input shape\n",
    "        reshaped_error = self.reshape(output_error, self.last_input_shape)\n",
    "        return reshaped_error\n",
    "\n",
    "    def flatten(self, input_data):\n",
    "        # Recursively flatten the input data\n",
    "        if isinstance(input_data[0], list):\n",
    "            return [item for sublist in input_data for item in self.flatten(sublist)]\n",
    "        else:\n",
    "            return input_data\n",
    "\n",
    "    def reshape(self, output_error, shape):\n",
    "        # Recursively build the nested lists with the given shape\n",
    "        if len(shape) == 1:\n",
    "            return output_error\n",
    "        else:\n",
    "            step = len(output_error) // shape[0]\n",
    "            return [self.reshape(output_error[i * step:(i + 1) * step], shape[1:]) for i in range(shape[0])]\n",
    "\n",
    "    def get_shape(self, lst):\n",
    "        # Get the shape of nested lists\n",
    "        shape = []\n",
    "        while isinstance(lst, list):\n",
    "            shape.append(len(lst))\n",
    "            lst = lst[0] if lst else None\n",
    "        return tuple(shape)\n",
    "        \n",
    "# Dropout Layer\n",
    "class DropoutLayer:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Create a mask with the same shape as the input data\n",
    "        self.mask = [[random.uniform(0, 1) > self.dropout_rate for _ in neuron] for neuron in input_data]\n",
    "        # Apply the mask to the input data\n",
    "        dropped_out_data = [[neuron[i] * self.mask[neuron_index][i] for i in range(len(neuron))] for neuron_index, neuron in enumerate(input_data)]\n",
    "        return dropped_out_data\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Apply the mask to the output error\n",
    "        if self.mask is not None:\n",
    "            return [[output_error[neuron_index][i] * self.mask[neuron_index][i] for i in range(len(neuron))] for neuron_index, neuron in enumerate(output_error)]\n",
    "        else:\n",
    "            return output_error\n",
    "\n",
    "# Dense Layer\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.biases = [0 for _ in range(output_size)]\n",
    "        self.last_input = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        # Perform matrix multiplication and add biases\n",
    "        output = [self.vector_dot_product(input_data, col) + self.biases[i] for i, col in enumerate(zip(*self.weights))]\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # Calculate gradients for weights and biases\n",
    "        d_weights = [[self.last_input[i] * output_error[j] for j in range(len(output_error))] for i in range(len(self.last_input))]\n",
    "        d_biases = output_error\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(len(self.weights[i])):\n",
    "                self.weights[i][j] -= learning_rate * d_weights[i][j]\n",
    "        for i in range(len(self.biases)):\n",
    "            self.biases[i] -= learning_rate * d_biases[i]\n",
    "\n",
    "        # Calculate input error\n",
    "        input_error = [self.vector_dot_product(row, output_error) for row in self.weights]\n",
    "        return input_error\n",
    "\n",
    "    def vector_dot_product(self, vec1, vec2):\n",
    "        # Calculate dot product of two vectors\n",
    "        return sum(x * y for x, y in zip(vec1, vec2))\n",
    "        \n",
    "# Softmax Layer\n",
    "class SoftmaxLayer:\n",
    "    def forward(self, input_data):\n",
    "        # Assuming input_data is a list of lists (2D array)\n",
    "        # Each inner list represents a single data sample\n",
    "        \n",
    "        # Clip values to a reasonable range for numerical stability\n",
    "        clipped_input = self.clip(input_data, -1e2, 1e2)\n",
    "        \n",
    "        # Calculate the softmax for each sample\n",
    "        softmax_output = [self.compute_softmax(sample) for sample in clipped_input]\n",
    "        return softmax_output\n",
    "\n",
    "    def compute_softmax(self, sample):\n",
    "        # Subtract the max value from each element for numerical stability\n",
    "        max_val = max(sample)\n",
    "        shifted_inputs = [x - max_val for x in sample]\n",
    "        \n",
    "        # Calculate the exponentials using math.exp\n",
    "        exps = [math.exp(x) for x in shifted_inputs]\n",
    "        \n",
    "        # Sum of the exponentials\n",
    "        sum_of_exps = sum(exps)\n",
    "        \n",
    "        # Normalize each value\n",
    "        return [exp_val / sum_of_exps for exp_val in exps]\n",
    "\n",
    "    def clip(self, matrix, min_value, max_value):\n",
    "        # Clip the values of a 2D list element-wise\n",
    "        return [[max(min_value, min(value, max_value)) for value in row] for row in matrix]\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # Error is passed straight through to the next layer\n",
    "        return output_error\n",
    "        \n",
    "# ReLU Activation\n",
    "class ReLULayer:\n",
    "    def forward(self, input_data):\n",
    "        # Apply ReLU to each element in the input data recursively\n",
    "        self.last_input = input_data\n",
    "        return self.apply_relu(input_data)\n",
    "\n",
    "    def apply_relu(self, data):\n",
    "        # Recursively apply ReLU to data of any dimension\n",
    "        if isinstance(data, list):\n",
    "            return [self.apply_relu(sub_data) for sub_data in data]\n",
    "        else:\n",
    "            return max(0, data)\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Compute the gradient of ReLU for the last input\n",
    "        return self.compute_gradient(output_error, self.last_input)\n",
    "\n",
    "    def compute_gradient(self, error, last_input):\n",
    "        # Recursively compute the gradient of ReLU for data of any dimension\n",
    "        if isinstance(error, list):\n",
    "            return [self.compute_gradient(sub_error, sub_last_input)\n",
    "                    for sub_error, sub_last_input in zip(error, last_input)]\n",
    "        else:\n",
    "            return error * (1 if last_input > 0 else 0)\n",
    "        \n",
    "# Cross-Entropy Loss\n",
    "class CrossEntropyLoss:\n",
    "    def calculate_loss(self, predicted, actual):\n",
    "        # Assuming 'predicted' and 'actual' are lists of lists\n",
    "        # Each inner list represents the probabilities for a single data sample\n",
    "        # 'actual' is expected to be one-hot encoded\n",
    "        \n",
    "        # Calculate the cross-entropy loss for each sample\n",
    "        losses = [self.compute_sample_loss(p, a) for p, a in zip(predicted, actual)]\n",
    "        \n",
    "        # Calculate the average loss over all samples\n",
    "        return sum(losses) / len(losses)\n",
    "\n",
    "    def compute_sample_loss(self, predicted_sample, actual_sample):\n",
    "        # Calculate the cross-entropy loss for a single sample\n",
    "        # Add a small constant (epsilon) to prevent log(0)\n",
    "        epsilon = 1e-9\n",
    "        return -sum([act * self.safe_log(pred + epsilon) for pred, act in zip(predicted_sample, actual_sample)])\n",
    "\n",
    "    def safe_log(self, x):\n",
    "        # Compute the natural log safely\n",
    "        if x <= 0:\n",
    "            return -float('inf')  # Logarithm of non-positive number is undefined\n",
    "        else:\n",
    "            return math.log(x)\n",
    "\n",
    "    def calculate_gradient(self, predicted, actual):\n",
    "        # Assuming 'predicted' and 'actual' are lists of lists\n",
    "        # Each inner list represents the probabilities for a single data sample\n",
    "        # 'actual' is expected to be one-hot encoded\n",
    "        \n",
    "        # Calculate the gradient of the loss with respect to the predictions\n",
    "        gradients = [self.compute_sample_gradient(p, a) for p, a in zip(predicted, actual)]\n",
    "        return gradients\n",
    "\n",
    "    def compute_sample_gradient(self, predicted_sample, actual_sample):\n",
    "        # Calculate the gradient for a single sample\n",
    "        # Add a small constant (epsilon) to prevent division by zero\n",
    "        epsilon = 1e-9\n",
    "        return [(pred - act) / (pred + epsilon) for pred, act in zip(predicted_sample, actual_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_classes, epochs=10, learning_rate=0.001, threshold=0.5, early_stopping_patience=5):\n",
    "        self.layers = [\n",
    "            ConvLayer(num_filters=32, filter_size=3, num_channels=3),  # num_channels is 3 for RGB images\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            ConvLayer(num_filters=64, filter_size=3, num_channels=32),  # num_channels is num_filters of previous ConvLayer\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            ConvLayer(num_filters=128, filter_size=3, num_channels=64),  # num_channels is num_filters of previous ConvLayer\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            ConvLayer(num_filters=256, filter_size=3, num_channels=128),  # num_channels is num_filters of previous ConvLayer\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            ConvLayer(num_filters=512, filter_size=3, num_channels=256),  # num_channels is num_filters of previous ConvLayer\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            FlattenLayer(),\n",
    "            None,  # Placeholder for first DenseLayer, will initialize properly later\n",
    "            ReLULayer(),\n",
    "            DropoutLayer(dropout_rate=0.5),\n",
    "            None,  # Placeholder for second DenseLayer, will initialize properly later\n",
    "            ReLULayer(),\n",
    "            DropoutLayer(dropout_rate=0.5),\n",
    "            None,  # Placeholder for third DenseLayer, will initialize properly later\n",
    "            SoftmaxLayer()\n",
    "        ] \n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_loss = np.inf\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    # Function to replace np.expand_dims()\n",
    "    def expand_dims(self, lst, axis):\n",
    "        if axis == 0:\n",
    "            return [lst]\n",
    "        else:\n",
    "            return [[item] for item in lst]\n",
    "    \n",
    "    # Function to replace np.array()\n",
    "    def to_list(self, nested_iterables):\n",
    "        return [to_list(item) if isinstance(item, (list, tuple)) else item for item in nested_iterables]\n",
    "    \n",
    "    # Function to perform element-wise multiplication\n",
    "    def elementwise_multiply(self, lst1, lst2):\n",
    "        if isinstance(lst1[0], list):\n",
    "            return [elementwise_multiply(sub1, sub2) for sub1, sub2 in zip(lst1, lst2)]\n",
    "        else:\n",
    "            return [a * b for a, b in zip(lst1, lst2)]\n",
    "    \n",
    "    # Function to perform dot product\n",
    "    def dot_product(self, vec1, vec2):\n",
    "        return sum(x * y for x, y in zip(vec1, vec2))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            if layer is not None:\n",
    "                # Ensure X is a list of lists\n",
    "                if not isinstance(X, list):\n",
    "                    X = self.to_list(X)\n",
    "                print(f\"+ Before forward - {type(layer).__name__}: {self.get_shape(X)}\")\n",
    "                X = layer.forward(X)\n",
    "                print(f\"- After forward - {type(layer).__name__}: {self.get_shape(X)}\")\n",
    "        return X\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            if layer is not None:\n",
    "                print(f\"+ Before backward - {type(layer).__name__}: {output_error.shape}\")\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "                print(f\"- After backward - {type(layer).__name__}: {output_error.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'last_input'):\n",
    "                layer.last_input = None\n",
    "    \n",
    "    def validate(self, X_val, y_val):\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        # Iterate over each example in the validation set\n",
    "        for i in range(len(X_val)):\n",
    "            # Forward pass\n",
    "            input_data = [X_val[i]]  # Mimic np.expand_dims\n",
    "            output = self.forward(input_data)\n",
    "        \n",
    "            # Calculate loss for the current validation example\n",
    "            val_loss = self.loss_function(output, y_val[i])\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Calculate accuracy for the current validation example\n",
    "            predicted_label = output.index(max(output))\n",
    "            val_accuracy = 1 if predicted_label == y_val[i] else 0\n",
    "            val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Calculate average loss and accuracy over the entire validation set\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "        \n",
    "        return avg_val_loss, avg_val_accuracy\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        predictions = self.forward(X)\n",
    "        return self.loss_function(predictions, y)\n",
    "    \n",
    "    def calculate_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        correct_predictions = sum(1 for pred, actual in zip(predictions, y) if pred == actual)\n",
    "        return correct_predictions / len(y)\n",
    "    \n",
    "    def min_max_scale(self, data):\n",
    "        min_val = min(data)\n",
    "        max_val = max(data)\n",
    "        range_val = max_val - min_val\n",
    "        if range_val == 0:\n",
    "            return [0 for _ in data]  # Return a list of zeros if all values are the same\n",
    "        scaled_data = [(value - min_val) / range_val for value in data]\n",
    "        return scaled_data\n",
    "\n",
    "    def get_shape(self, lst):\n",
    "        shape = []\n",
    "        while isinstance(lst, list):\n",
    "            shape.append(len(lst))\n",
    "            lst = lst[0] if lst else None\n",
    "        return tuple(shape)\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        print(f\"Initial training data shape: {X_train[0].shape}\\n\")\n",
    "        \n",
    "        # Calculate the output size after the convolution and pooling layers\n",
    "        sample_output = self.forward(np.expand_dims(X_train[0], axis=0))\n",
    "        flattened_size = sample_output.size\n",
    "        \n",
    "        # Initialize the DenseLayers with the correct input size\n",
    "        self.layers[-6] = DenseLayer(input_size=flattened_size, output_size=128)  # First DenseLayer\n",
    "        self.layers[-3] = DenseLayer(input_size=128, output_size=64)  # Second DenseLayer\n",
    "        self.layers[-1] = DenseLayer(input_size=64, output_size=self.num_classes)  # Third DenseLayer\n",
    "\n",
    "        print(\"\\n  |====== Start training ======| \\n\")\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for j in range(len(X_train)):\n",
    "                print(f\"\\n=== start round {j}/{len(X_train)-1} - epoch {epoch} ===\\n\")\n",
    "                \n",
    "                # forward\n",
    "                input_data = [X_train[j]]  # Mimic np.expand_dims\n",
    "                output = self.forward(input_data)\n",
    "\n",
    "                # print(\"---------------------------------------\")\n",
    "                \n",
    "                # backward\n",
    "                output_error = self.loss.calculate_gradient(output, y_train[j])\n",
    "                # Clip output error\n",
    "                threshold = 1e2  # Define an appropriate threshold\n",
    "                output_error = [[self.clip(value, -threshold, threshold) for value in row] for row in output_error]\n",
    "                output_error = self.backward(output_error, self.learning_rate)\n",
    "                                \n",
    "                # Calculate loss and accuracy after the backward pass\n",
    "                batch_loss = self.loss.calculate_loss(output, y_train[j])\n",
    "                batch_losses.append(batch_loss)\n",
    "                predicted_label = output.index(max(output))\n",
    "                batch_accuracy = 1 if predicted_label == y_train[j] else 0\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "            \n",
    "                self.reset()  # Reset the network's state after each forward pass\n",
    "                print(f\"\\n=== end round {j} - epoch {epoch} ===\\n\")\n",
    "\n",
    "            print(f\"\\n=== end epoch {epoch} ===\\n\")\n",
    "            \n",
    "            # Calculate the average training loss and accuracy for the epoch\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_loss = self.min_max_scale(avg_train_loss)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "            avg_train_accuracy = self.min_max_scale(avg_train_accuracy)\n",
    "            \n",
    "            # Append the average metrics to the lists that track them across epochs\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracies.append(avg_train_accuracy)\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "            # Validation step\n",
    "            val_loss, val_accuracy = self.validate(X_val, y_val)\n",
    "            val_loss = self.min_max_scale(val_loss)\n",
    "            val_accuracy = self.min_max_scale(val_accuracy)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss}, Train Accuracy: {avg_train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
    "    \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            if self.patience_counter >= self.early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        print(f\"Best validation accuracy achieved: {self.best_loss}\")\n",
    "\n",
    "        self.plot_results(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass without NumPy\n",
    "            input_data = [X[i]]  # Mimic np.expand_dims\n",
    "            output = self.forward(input_data)\n",
    "            predicted_label = output.index(max(output))  # Use pure Python to get the class index\n",
    "            predictions.append(predicted_label)\n",
    "        return predictions\n",
    "    \n",
    "    def plot_results(self, train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "        actual_epochs = min(len(train_losses), len(train_accuracies), len(val_losses), len(val_accuracies))\n",
    "        assert actual_epochs > 0, \"No data to plot.\"\n",
    "    \n",
    "        # If 'val_losses' is a nested list, flatten it\n",
    "        if isinstance(val_losses[0], list):\n",
    "            val_losses = [loss for sublist in val_losses for loss in sublist]\n",
    "        \n",
    "        epochs = range(1, actual_epochs + 1)\n",
    "    \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses[:actual_epochs], 'g', label='Training loss')\n",
    "        plt.plot(epochs, val_losses[:actual_epochs], 'b', label='Validation loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_accuracies[:actual_epochs], 'g', label='Training accuracy')\n",
    "        plt.plot(epochs, val_accuracies[:actual_epochs], 'b', label='Validation accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "120993d8-0d53-4c5f-9d8c-2dc201c6e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training data shape: (150, 150, 3)\n",
      "\n",
      "+ Before forward - ConvLayer: (1,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m early_stopping_patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      8\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN(num_classes, epochs, learning_rate, threshold, early_stopping_patience)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 137\u001b[0m, in \u001b[0;36mCNN.train\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial training data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Calculate the output size after the convolution and pooling layers\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m sample_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m flattened_size \u001b[38;5;241m=\u001b[39m sample_output\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Initialize the DenseLayers with the correct input size\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 67\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m             X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_list(X)\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+ Before forward - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_shape(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- After forward - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_shape(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "Cell \u001b[0;32mIn[27], line 73\u001b[0m, in \u001b[0;36mConvLayer.forward\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     70\u001b[0m patch \u001b[38;5;241m=\u001b[39m [row[horizontal_start:horizontal_end] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m input_padded[vertical_start:vertical_end]]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Perform element-wise multiplication and sum the result\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m multiplied_patch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_wise_multiplication\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m conv_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_matrix(multiplied_patch)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Clip the values and assign to the output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 34\u001b[0m, in \u001b[0;36mConvLayer.element_wise_multiplication\u001b[0;34m(self, matrix1, matrix2)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melement_wise_multiplication\u001b[39m(\u001b[38;5;28mself\u001b[39m, matrix1, matrix2):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Assuming matrix1 and matrix2 are 2D lists of the same size\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmatrix2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melement_wise_multiplication\u001b[39m(\u001b[38;5;28mself\u001b[39m, matrix1, matrix2):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Assuming matrix1 and matrix2 are 2D lists of the same size\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m[\u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmatrix2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matrix1))]\n",
      "Cell \u001b[0;32mIn[27], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melement_wise_multiplication\u001b[39m(\u001b[38;5;28mself\u001b[39m, matrix1, matrix2):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Assuming matrix1 and matrix2 are 2D lists of the same size\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[\u001b[43mmatrix1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmatrix2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matrix1[\u001b[38;5;241m0\u001b[39m]))] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matrix1))]\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "# Adjust the values to train\n",
    "num_classes = num_classes\n",
    "epochs = 1\n",
    "learning_rate = 0.1\n",
    "threshold = 0.5\n",
    "early_stopping_patience = 5\n",
    "\n",
    "cnn = CNN(num_classes, epochs, learning_rate, threshold, early_stopping_patience)\n",
    "cnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337c04-c9f5-413a-87b8-72d93bdfe88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
