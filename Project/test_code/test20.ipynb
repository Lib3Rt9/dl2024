{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import numpy as np # to process the data\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install h5py\n",
    "import h5py\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image .ipynb_checkpoints: [Errno 21] Is a directory: '/storage/student14/phucpg/WORKSPACE/flowers/.ipynb_checkpoints'\n",
      "Total number of images: 733\n",
      "Dimensions of each image: (80, 80, 3)\n",
      "Total dimensions of the dataset (assuming all images have the same shape): 733 x (80, 80, 3)\n",
      "Train set size: 549, Validation set size: 109, Test set size: 75\n",
      "Unique classes in the training set: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Number of classes for prediction: 10\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory, image_size=(80, 80)):\n",
    "        self.directory = directory\n",
    "        self.image_size = image_size\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        image_files = os.listdir(self.directory)\n",
    "        for img in image_files:\n",
    "        # for img in image_files[:20]:\n",
    "            try:\n",
    "                img_path = os.path.join(self.directory, img)\n",
    "                with Image.open(img_path) as img_array:\n",
    "                    # Convert image to RGB\n",
    "                    img_array = img_array.convert(\"RGB\").resize(self.image_size)\n",
    "                    img_data = [[list(img_array.getpixel((x, y))) for y in range(self.image_size[1])] for x in range(self.image_size[0])]\n",
    "                    self.images.append(img_data)\n",
    "                    label = img.split('_')[0]\n",
    "                    self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        max_pixel_value = 255.0\n",
    "        self.images = [[[[value / max_pixel_value for value in pixel] for pixel in row] for row in image] for image in self.images]\n",
    "\n",
    "    def print_dataset_dimensions(self):\n",
    "        if self.images:\n",
    "            print(f\"Total number of images: {len(self.images)}\")\n",
    "            print(f\"Dimensions of each image: ({len(self.images[0])}, {len(self.images[0][0])}, {len(self.images[0][0][0])})\")\n",
    "            print(f\"Total dimensions of the dataset (assuming all images have the same shape): {len(self.images)} x ({len(self.images[0])}, {len(self.images[0][0])}, {len(self.images[0][0][0])})\")\n",
    "        else:\n",
    "            print(\"The dataset is empty or not loaded properly.\")\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        label_to_int = {label: index for index, label in enumerate(unique_labels)}\n",
    "        self.label_indices = [label_to_int[label] for label in self.labels]\n",
    "        self.labels = [[int(i == label_index) for i in range(len(unique_labels))] for label_index in self.label_indices]\n",
    "\n",
    "def split_data(images, labels, train_ratio, validation_ratio, test_ratio):\n",
    "    combined = list(zip(images, labels))\n",
    "    shuffle(combined)\n",
    "    shuffled_images, shuffled_labels = zip(*combined)\n",
    "\n",
    "    train_end = int(len(shuffled_images) * train_ratio)\n",
    "    validation_end = train_end + int(len(shuffled_images) * validation_ratio)\n",
    "\n",
    "    x_train = shuffled_images[:train_end]\n",
    "    y_train = shuffled_labels[:train_end]\n",
    "    x_val = shuffled_images[train_end:validation_end]\n",
    "    y_val = shuffled_labels[train_end:validation_end]\n",
    "    x_test = shuffled_images[validation_end:]\n",
    "    y_test = shuffled_labels[validation_end:]\n",
    "\n",
    "    return list(x_train), list(x_val), list(x_test), list(y_train), list(y_val), list(y_test)\n",
    "\n",
    "# Usage\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "dataset.one_hot_encode()\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(dataset.images, dataset.labels, 0.75, 0.15, 0.10)\n",
    "\n",
    "# print(\"y_train after one hot encoded: \", y_train)\n",
    "\n",
    "# Print the dimensions of the preprocessed images\n",
    "dataset.print_dataset_dimensions()\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = sorted(set([label.index(1) for label in y_train]))\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")\n",
    "\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Number of classes for prediction: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce61edb0-96c3-4537-9f3b-bace8ce3a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Limiting:\n",
    "    @staticmethod\n",
    "    def clip(values, min_value, max_value):\n",
    "        # Check if values is a list or a single value\n",
    "        if isinstance(values, list):\n",
    "            return [max(min(value, max_value), min_value) for value in values]\n",
    "        else:\n",
    "            return max(min(values, max_value), min_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_filename(base_filename, extension):\n",
    "        # Start with 0 suffix and increment until a unique filename is found\n",
    "        counter = 0\n",
    "        unique_filename = f\"{base_filename}_{counter}.{extension}\"\n",
    "        while os.path.exists(unique_filename):\n",
    "            counter += 1\n",
    "            unique_filename = f\"{base_filename}_{counter}.{extension}\"\n",
    "        return unique_filename\n",
    "\n",
    "    @staticmethod\n",
    "    def min_max_scale(data):\n",
    "        min_val = min(data)\n",
    "        max_val = max(data)\n",
    "        range_val = max_val - min_val\n",
    "        if range_val == 0:\n",
    "            return [0 for _ in data]  # Return a list of zeros if all values are the same\n",
    "        scaled_data = [(value - min_val) / range_val for value in data]\n",
    "        return scaled_data\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_one_hot(predicted):\n",
    "        one_hot_predicted = []\n",
    "        for pred in predicted:\n",
    "            # Find the index with the maximum probability\n",
    "            max_index = pred.index(max(pred))\n",
    "            # Create a one-hot encoded vector\n",
    "            one_hot_vector = [0] * len(pred)\n",
    "            one_hot_vector[max_index] = 1\n",
    "            one_hot_predicted.append(one_hot_vector)\n",
    "        return one_hot_predicted\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_to_class_index(one_hot_vector):\n",
    "        return one_hot_vector.index(1)\n",
    "\n",
    "class Precision:\n",
    "    def calculate(self, predicted, actual):\n",
    "        tp = sum(p == a == 1 for p, a in zip(predicted, actual))\n",
    "        fp = sum(p == 1 and a == 0 for p, a in zip(predicted, actual))\n",
    "        return tp / (tp + fp) if tp + fp > 0 else 0\n",
    "\n",
    "class Recall:\n",
    "    def calculate(self, predicted, actual):\n",
    "        tp = sum(p == a == 1 for p, a in zip(predicted, actual))\n",
    "        fn = sum(p == 0 and a == 1 for p, a in zip(predicted, actual))\n",
    "        return tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "class F1Score:\n",
    "    def calculate(self, predicted, actual):\n",
    "        precision = Precision().calculate(predicted, actual)\n",
    "        recall = Recall().calculate(predicted, actual)\n",
    "        return 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer class\n",
    "class Layer:\n",
    "    def forward(self, input_data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Convolutional Layer\n",
    "class ConvLayer(Layer):\n",
    "    def __init__(self, num_filters, filter_size, num_channels=3, padding=0, stride=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[[[random.random() for _ in range(num_channels)] for _ in range(filter_size)] for _ in range(filter_size)] for _ in range(num_filters)]\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # # Assert that the input data has the expected shape\n",
    "        # assert len(input_data.shape) == 4, f\"Expected input shape to be 4D, got {input_data.shape}\"\n",
    "        # # Ensure the input data has four dimensions: batch size, height, width, and depth\n",
    "        # if len(input_data.shape) < 4:\n",
    "        #     input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "        self.last_input = input_data\n",
    "        batch_size, h, w, d = len(input_data), len(input_data[0]), len(input_data[0][0]), len(input_data[0][0][0])\n",
    "    \n",
    "        new_h = (h + 2 * self.padding - self.filter_size) // self.stride + 1\n",
    "        new_w = (w + 2 * self.padding - self.filter_size) // self.stride + 1\n",
    "        output = [[[[0 for _ in range(self.num_filters)] for _ in range(new_w)] for _ in range(new_h)] for _ in range(batch_size)]\n",
    "    \n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, new_h, self.stride):\n",
    "                for j in range(0, new_w, self.stride):\n",
    "                    for f in range(self.num_filters):\n",
    "                        h_start = i*self.stride\n",
    "                        h_end = h_start+self.filter_size\n",
    "                        w_start = j*self.stride\n",
    "                        w_end = w_start+self.filter_size\n",
    "    \n",
    "                        patch = [[[input_data[b][x][y][c] for c in range(d)] for y in range(w_start, w_end)] for x in range(h_start, h_end)]\n",
    "                        output_value = sum(sum(sum(a*b for a, b in zip(patch_row, filter_row)) for patch_row, filter_row in zip(patch_channel, filter_channel)) for patch_channel, filter_channel in zip(patch, self.filters[f]))\n",
    "                        # output[b][i][j][f] = Limiting.clip(output_value, 1e - 9, 1 - (1e -9))  # Clip the output value using Limiting.clip\n",
    "                        output[b][i][j][f] = output_value\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        self.last_input = self.last_input\n",
    "        batch_size, h, w, d = len(self.last_input), len(self.last_input[0]), len(self.last_input[0][0]), len(self.last_input[0][0][0])\n",
    "    \n",
    "        new_h = (h + 2 * self.padding - self.filter_size) // self.stride + 1\n",
    "        new_w = (w + 2 * self.padding - self.filter_size) // self.stride + 1\n",
    "        output = [[[[0 for _ in range(self.num_filters)] for _ in range(new_w)] for _ in range(new_h)] for _ in range(batch_size)]\n",
    "    \n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, new_h, self.stride):\n",
    "                for j in range(0, new_w, self.stride):\n",
    "                    for f in range(self.num_filters):\n",
    "                        h_start = i*self.stride\n",
    "                        h_end = h_start+self.filter_size\n",
    "                        w_start = j*self.stride\n",
    "                        w_end = w_start+self.filter_size\n",
    "    \n",
    "                        patch = [[self.last_input[b][x][y] for y in range(w_start, w_end)] for x in range(h_start, h_end)]\n",
    "                        output_value = sum(sum(sum(a*b for a, b in zip(patch_row, filter_row)) for patch_row, filter_row in zip(patch_channel, filter_channel)) for patch_channel, filter_channel in zip(patch, self.filters[f]))\n",
    "                        output[b][i][j][f] = Limiting.clip(output_value, -10**9, 10**9)  # Clip the output value using Limiting.clip\n",
    "    \n",
    "        return output\n",
    "        \n",
    "# ReLU Activation\n",
    "class ReLULayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        return self.apply_relu(input_data)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return self.apply_relu_gradient(output_error)\n",
    "\n",
    "    def apply_relu(self, data):\n",
    "        # Apply ReLU to each value in the input data\n",
    "        return [[[[max(0, val) for val in channel] for channel in depth] for depth in width] for width in data]\n",
    "\n",
    "    def apply_relu_gradient(self, error):\n",
    "        # Apply the gradient of ReLU to the output error\n",
    "        return [[[[err * (val > 0) for err, val in zip(channel_err, channel_val)]\n",
    "                  for channel_err, channel_val in zip(depth_err, depth_val)]\n",
    "                 for depth_err, depth_val in zip(width_err, width_val)]\n",
    "                for width_err, width_val in zip(error, self.last_input)]\n",
    "\n",
    "# MaxPooling Layer\n",
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.last_input = input_data\n",
    "        batch_size, h, w, num_filters = len(input_data), len(input_data[0]), len(input_data[0][0]), len(input_data[0][0][0])\n",
    "        output = [[[[0 for _ in range(num_filters)] for _ in range(w // self.pool_size)] for _ in range(h // self.pool_size)] for _ in range(batch_size)]\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h // self.pool_size):\n",
    "                for j in range(w // self.pool_size):\n",
    "                    for f in range(num_filters):\n",
    "                        patch = [[input_data[b][x][y][f] for y in range(j*self.pool_size, (j+1)*self.pool_size)] for x in range(i*self.pool_size, (i+1)*self.pool_size)]\n",
    "                        max_value = max(max(row) for row in patch)\n",
    "                        # output[b][i][j][f] = Limiting.clip(max_value, 1e - 9 , 1 - (1e - 9))  # Clip the output value using Limiting.clip\n",
    "                        output[b][i][j][f] = max_value\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # Initialize d_input with zeros, having the same shape as the last input\n",
    "        d_input = [[[[\n",
    "            0 for _ in range(len(self.last_input[0][0][0]))] \n",
    "            for _ in range(len(self.last_input[0][0]))] \n",
    "            for _ in range(len(self.last_input[0]))] \n",
    "            for _ in range(len(self.last_input))]\n",
    "\n",
    "        for b in range(len(self.last_input)):\n",
    "            for h in range(0, len(self.last_input[b]), self.pool_size):\n",
    "                for w in range(0, len(self.last_input[b][h]), self.pool_size):\n",
    "                    for c in range(len(self.last_input[b][h][w])):\n",
    "                        # Find the max value in the pooling window\n",
    "                        max_value = None\n",
    "                        max_h = 0\n",
    "                        max_w = 0\n",
    "                        for i in range(self.pool_size):\n",
    "                            for j in range(self.pool_size):\n",
    "                                current_h = h + i\n",
    "                                current_w = w + j\n",
    "                                if current_h < len(self.last_input[b]) and current_w < len(self.last_input[b][current_h]):\n",
    "                                    if max_value is None or self.last_input[b][current_h][current_w][c] > max_value:\n",
    "                                        max_value = self.last_input[b][current_h][current_w][c]\n",
    "                                        max_h = current_h\n",
    "                                        max_w = current_w\n",
    "                        # Propagate the error to the max location\n",
    "                        if h//self.pool_size < len(output_error[b]) and w//self.pool_size < len(output_error[b][h//self.pool_size]):\n",
    "                            d_input[b][max_h][max_w][c] = output_error[b][h//self.pool_size][w//self.pool_size][c]\n",
    "\n",
    "        return d_input\n",
    "        \n",
    "class FlattenLayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        self.last_input_shape = [len(input_data), len(input_data[0]), len(input_data[0][0]), len(input_data[0][0][0])]\n",
    "        flattened = []\n",
    "        \n",
    "        for b in range(self.last_input_shape[0]):\n",
    "            for i in range(self.last_input_shape[1]):\n",
    "                for j in range(self.last_input_shape[2]):\n",
    "                    for k in range(self.last_input_shape[3]):\n",
    "                        value = input_data[b][i][j][k]\n",
    "                        # clipped_value = Limiting.clip(value, -10**6, 10**6)  # Clip the value using Limiting.clip\n",
    "                        # flattened.append(clipped_value)\n",
    "                        flattened.append(value)\n",
    "                        \n",
    "        return [flattened]  # Return a list of lists\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        reshaped_error = []\n",
    "        \n",
    "        for b in range(self.last_input_shape[0]):\n",
    "            image_error = []\n",
    "            \n",
    "            for i in range(self.last_input_shape[1]):\n",
    "                row_error = []\n",
    "                \n",
    "                for j in range(self.last_input_shape[2]):\n",
    "                    pixel_error = []\n",
    "                    \n",
    "                    for k in range(self.last_input_shape[3]):\n",
    "                        error_value = output_error[0][b*self.last_input_shape[1]*self.last_input_shape[2]*self.last_input_shape[3] + i*self.last_input_shape[2]*self.last_input_shape[3] + j*self.last_input_shape[3] + k]\n",
    "                        pixel_error.append(error_value)\n",
    "                        \n",
    "                    row_error.append(pixel_error)\n",
    "                image_error.append(row_error)\n",
    "            reshaped_error.append(image_error)\n",
    "            \n",
    "        return reshaped_error\n",
    "        \n",
    "# Dropout Layer\n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.mask = self.create_dropout_mask(input_data)\n",
    "        return self.apply_dropout_mask(input_data)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return self.apply_dropout_mask(output_error)\n",
    "\n",
    "    def create_dropout_mask(self, input_data):\n",
    "        mask = []\n",
    "        for i in range(len(input_data)):\n",
    "            mask_row = self.create_mask_row(len(input_data[0]))\n",
    "            mask.append(mask_row)\n",
    "        return mask\n",
    "\n",
    "    def create_mask_row(self, length):\n",
    "        return [(random.random() > self.dropout_rate) / (1.0 - self.dropout_rate) for _ in range(length)]\n",
    "\n",
    "    def apply_dropout_mask(self, input_data):\n",
    "        output = []\n",
    "        for i in range(len(input_data)):\n",
    "            output_row = self.apply_mask_row(i, input_data[i])\n",
    "            output.append(output_row)\n",
    "        return output\n",
    "\n",
    "    def apply_mask_row(self, i, input_row):\n",
    "        return [self.mask[i][j] * input_row[j] for j in range(len(input_row))]\n",
    "\n",
    "# Dense Layer\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, name, input_size, output_size):\n",
    "        self.name = name\n",
    "        self.weights = [[random.random() for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.biases = [0 for _ in range(output_size)]  # Ensure biases is a 1D array with length 'output_size'\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Assert that the input data is 2D after flattening\n",
    "        # assert len(input_data.shape) == 2, f\"Expected input shape to be 2D, got {input_data.shape}\"\n",
    "        self.last_input = input_data\n",
    "        output = []\n",
    "        for data in input_data:\n",
    "            output_row = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                value = 0\n",
    "                for d, w_ in zip(data, w):\n",
    "                    value += d * w_\n",
    "                value += b\n",
    "                output_row.append(value)\n",
    "            output.append(output_row)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        d_weights = []\n",
    "        for j in range(len(self.weights)):\n",
    "            d_weights_row = []\n",
    "            for k in range(len(self.weights[0])):\n",
    "                d_weight = 0\n",
    "                for i in range(len(self.last_input)):\n",
    "                    d_weight += self.last_input[i][j] * output_error[i][k]\n",
    "                d_weights_row.append(d_weight)\n",
    "            d_weights.append(d_weights_row)\n",
    "    \n",
    "        d_biases = []\n",
    "        for j in range(len(self.biases)):\n",
    "            d_bias = 0\n",
    "            for i in range(len(output_error)):\n",
    "                d_bias += output_error[i][j]\n",
    "            d_biases.append(d_bias)\n",
    "    \n",
    "        for j in range(len(self.weights)):\n",
    "            for k in range(len(self.weights[0])):\n",
    "                self.weights[j][k] -= learning_rate * d_weights[j][k]\n",
    "    \n",
    "        for j in range(len(self.biases)):\n",
    "            self.biases[j] -= learning_rate * d_biases[j]\n",
    "    \n",
    "        d_input = []\n",
    "        for i in range(len(output_error)):\n",
    "            d_input_row = []\n",
    "            for j in range(len(self.weights)):\n",
    "                d_input_value = 0\n",
    "                for k in range(len(self.weights[0])):\n",
    "                    d_input_value += self.weights[j][k] * output_error[i][k]\n",
    "                d_input_row.append(d_input_value)\n",
    "            d_input.append(d_input_row)\n",
    "    \n",
    "        return d_input\n",
    "        \n",
    "class SoftmaxLayer(Layer):\n",
    "    def forward(self, input_data):\n",
    "        # # Reshape input_data to 2D if it's 1D\n",
    "        # if input_data.ndim == 1:\n",
    "        #     input_data = input_data.reshape(1, -1)\n",
    "\n",
    "        self.last_input = input_data\n",
    "        output = []\n",
    "        for data in input_data:\n",
    "            max_value = max(data)\n",
    "            exp_values = [math.exp(value - max_value) for value in data]  # Subtract max_value for numerical stability\n",
    "            sum_exp_values = sum(exp_values)\n",
    "            softmax_values = [exp_value / sum_exp_values for exp_value in exp_values]\n",
    "            output.append(softmax_values)\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return output_error  # Error is passed straight through to the next layer\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "class CrossEntropyLoss:\n",
    "    def calculate_loss(self, predicted, actual):\n",
    "        # print(predicted)\n",
    "        # print(actual)\n",
    "        total_loss = 0\n",
    "        for pred, act in zip(predicted, actual):\n",
    "            clipped_pred = [Limiting.clip(p, 1e-9, 1 - 1e-9) for p in pred]  # Clip the predicted values\n",
    "            loss = -sum(act_i * math.log(pred_i) for act_i, pred_i in zip(act, clipped_pred))\n",
    "            # loss = -sum(act_i * math.log(pred_i) for act_i, pred_i in zip(act, pred))\n",
    "            total_loss += loss\n",
    "        return total_loss\n",
    "        \n",
    "    def calculate_gradient(self, predicted, actual):\n",
    "        # print(predicted)\n",
    "        # print(actual)\n",
    "        gradients = []\n",
    "        for pred, act in zip(predicted, actual):\n",
    "            gradient = [p - a for p, a in zip(pred, act)]\n",
    "            gradients.append(gradient)\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_classes, epochs=10, learning_rate=0.001, threshold=0.5, early_stopping_patience=5):\n",
    "        self.layers = [\n",
    "            ConvLayer(num_filters=32, filter_size=3, num_channels=3),  # num_channels is 3 for RGB images\n",
    "            ReLULayer(),\n",
    "            MaxPoolingLayer(pool_size=2),\n",
    "            # ConvLayer(num_filters=64, filter_size=3, num_channels=32),  # num_channels is num_filters of previous ConvLayer\n",
    "            # ReLULayer(),\n",
    "            # MaxPoolingLayer(pool_size=2),\n",
    "            # ConvLayer(num_filters=128, filter_size=3, num_channels=64),  # num_channels is num_filters of previous ConvLayer\n",
    "            # ReLULayer(),\n",
    "            # MaxPoolingLayer(pool_size=2),\n",
    "            # ConvLayer(num_filters=256, filter_size=3, num_channels=128),  # num_channels is num_filters of previous ConvLayer\n",
    "            # ReLULayer(),\n",
    "            # MaxPoolingLayer(pool_size=2),\n",
    "            # ConvLayer(num_filters=512, filter_size=3, num_channels=256),  # num_channels is num_filters of previous ConvLayer\n",
    "            # ReLULayer(),\n",
    "            # MaxPoolingLayer(pool_size=2),\n",
    "            FlattenLayer(),\n",
    "            # None, # -6   # Placeholder for first DenseLayer, will initialize properly later\n",
    "            # ReLULayer(),\n",
    "            # DropoutLayer(dropout_rate=0.1),\n",
    "            # None, # -3   # Placeholder for second DenseLayer, will initialize properly later\n",
    "            # ReLULayer(),\n",
    "            DropoutLayer(dropout_rate=0.1),\n",
    "            None, # -1   # Placeholder for third DenseLayer, will initialize properly later\n",
    "            SoftmaxLayer()\n",
    "        ] \n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            if layer is not None:\n",
    "                # Ensure X is a NumPy array\n",
    "                # if isinstance(X, list):\n",
    "                #     X = np.array(X)\n",
    "                # print(f\"+ Before forward - {type(layer).__name__}: {self.get_shape(X)}\")\n",
    "                X = layer.forward(X)\n",
    "                # print(f\"- After forward - {type(layer).__name__}: {self.get_shape(X)}\")\n",
    "        return X\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            if layer is not None:\n",
    "                # print(f\"+ Before backward - {type(layer).__name__}: {self.get_shape(output_error)}\")\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "                # print(f\"- After backward - {type(layer).__name__}: {self.get_shape(output_error)}\")\n",
    "        return output_error\n",
    "\n",
    "    def get_shape(self, lst):\n",
    "        shape = []\n",
    "        while isinstance(lst, list):\n",
    "            shape.append(len(lst))\n",
    "            lst = lst[0] if lst else None\n",
    "        return tuple(shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'last_input'):\n",
    "                layer.last_input = None\n",
    "\n",
    "    def validate(self, X_val, y_val):\n",
    "        val_losses = []\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "    \n",
    "        # Iterate over each example in the validation set\n",
    "        for i in range(len(X_val)):\n",
    "            # Forward pass\n",
    "            input_data = [X_val[i]]  # Add a batch dimension\n",
    "            output = self.forward(input_data)\n",
    "    \n",
    "            # Convert one-hot encoded vector to class index for the actual class\n",
    "            actual_class_index = y_val[i].index(1)\n",
    "            # Convert the output probabilities to class index for the predicted class\n",
    "            predicted_class_index = max(range(len(output[0])), key=output[0].__getitem__)\n",
    "    \n",
    "            # Calculate loss for the current validation example\n",
    "            val_loss = self.loss.calculate_loss([output[0]], [y_val[i]])  # Pass the entire one-hot encoded list\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "            # Check if the prediction is correct\n",
    "            predicted_class = output[0].index(max(output[0]))\n",
    "            # print(\"predicted_class: \", predicted_class)\n",
    "            actual_class = y_val[i].index(1)\n",
    "            # print(\"actual_class: \", actual_class)\n",
    "            if predicted_class == actual_class:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "            # print(\"correct_predictions\", correct_predictions)\n",
    "            # print(\"total_predictions\", total_predictions)\n",
    "        \n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "        return avg_val_loss, avg_val_accuracy\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        return self.loss.calculate_loss(X, y)\n",
    "\n",
    "    def calculate_accuracy(self, X, y):\n",
    "        correct_predictions = 0\n",
    "        for pred, act in zip(X, y):  # Use X and y instead of predicted and actual\n",
    "            predicted_class = pred.index(max(pred))\n",
    "            actual_class = act.index(1)\n",
    "            if predicted_class == actual_class:\n",
    "                correct_predictions += 1\n",
    "        accuracy = correct_predictions / len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        print(f\"Initial training data shape: {self.get_shape(X_train[0])}\\n\")\n",
    "        \n",
    "        # Calculate the output size after the convolution and pooling layers\n",
    "        sample_output = self.forward([X_train[0]])  # Add a batch dimension\n",
    "        flattened_size = len(sample_output[0])\n",
    "        \n",
    "        # Initialize the DenseLayers with the correct input size\n",
    "        # self.layers[-6] = DenseLayer(input_size=flattened_size, output_size=128)  # First DenseLayer\n",
    "        # self.layers[-3] = DenseLayer(input_size=128, output_size=64)  # Second DenseLayer\n",
    "        # self.layers[-1] = DenseLayer(input_size=64, output_size=self.num_classes)  # Third DenseLayer\n",
    "        \n",
    "        self.layers[-2] = DenseLayer(name='dense_layer_1', input_size=flattened_size, output_size=self.num_classes)  # Last DenseLayer\n",
    "\n",
    "        print(\"\\n  |====== Start training ======| \\n\")\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for j in range(len(X_train)):\n",
    "                # print(f\"\\n=== start round {j}/{len(X_train)-1}/{epoch} ===\\n\")\n",
    "                \n",
    "                # forward\n",
    "                input_data = [X_train[j]]  # Add a batch dimension\n",
    "                output = self.forward(input_data)\n",
    "\n",
    "                # print(\"---------------------------------------\")\n",
    "                \n",
    "                # backward\n",
    "                output_error = self.loss.calculate_gradient([output[0]], [y_train[j]])  # Pass the entire one-hot encoded list\n",
    "                output_error = self.backward(output_error, self.learning_rate)\n",
    "                                \n",
    "                # Calculate loss after the backward pass\n",
    "                batch_loss = self.loss.calculate_loss([output[0]], [y_train[j]])  # Pass the entire one-hot encoded list\n",
    "                batch_losses.append(batch_loss)\n",
    "                # print(f\"{j} batch_loss {batch_loss}\")\n",
    "\n",
    "                # Calculate accuracy after the backward pass\n",
    "                batch_accuracy = self.calculate_accuracy([output[0]], [y_train[j]])\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "                # print(f\"{j} batch_accuracy {batch_accuracy}\")\n",
    "                # correct_predictions += batch_accuracy\n",
    "                # total_predictions += 1\n",
    "\n",
    "                # predicted_class = output[0].index(max(output[0]))\n",
    "                # actual_class = y_train[j].index(1)\n",
    "                # if predicted_class == actual_class:\n",
    "                #     correct_predictions += 1\n",
    "                # total_predictions += 1\n",
    "\n",
    "                # print(f\"{j} correct_predictions: {correct_predictions}\")\n",
    "                # print(f\"{j} total_predictions {total_predictions}\")\n",
    "                \n",
    "                self.reset()  # Reset the network's state after each forward pass\n",
    "                \n",
    "                # print(f\"\\n=== end round {j}/{len(X_train)-1}/{epoch} ===\\n\")\n",
    "\n",
    "            # print(\"correct_predictions: \", batch_accuracies)\n",
    "            # print(\"total_predictions\", len(batch_accuracies))\n",
    "            \n",
    "            # Calculate the average training loss and accuracy for the epoch\n",
    "            avg_train_loss = sum(batch_losses) / len(batch_losses)\n",
    "            avg_train_accuracy = sum(batch_accuracies) / len(batch_accuracies)\n",
    "\n",
    "            # Append the average metrics to the lists that track them across epochs\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracies.append(avg_train_accuracy)\n",
    "            \n",
    "            self.reset()\n",
    "\n",
    "            # Validation step\n",
    "            val_loss, val_accuracy = self.validate(X_val, y_val)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss}, Train Accuracy: {avg_train_accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
    "    \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            if self.patience_counter >= self.early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        print()\n",
    "        print(\"train_losses:\", train_losses)\n",
    "        print(\"train_accuracies:\", train_accuracies)\n",
    "        print(\"val_losses:\", val_losses)\n",
    "        print(\"val_accuracies:\", val_accuracies)\n",
    "        print()\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        print(f\"Best validation accuracy achieved: {self.best_loss}\")\n",
    "\n",
    "        self.plot_results(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            # Ensure the input data has four dimensions: batch size, height, width, and depth\n",
    "            input_data = [X[i]]  # Add a batch dimension\n",
    "            output = self.forward(input_data)\n",
    "            predictions.append(max(range(len(output[0])), key=output[0].__getitem__))  # Use max to get the class index\n",
    "        return predictions\n",
    "    \n",
    "    def plot_results(self, train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "        actual_epochs = min(len(train_losses), len(train_accuracies), len(val_losses), len(val_accuracies))\n",
    "        assert actual_epochs > 0, \"No data to plot.\"\n",
    "    \n",
    "        # If 'val_losses' is a nested list, flatten it\n",
    "        if isinstance(val_losses[0], list):\n",
    "            val_losses = [loss for sublist in val_losses for loss in sublist]\n",
    "        \n",
    "        epochs = range(1, actual_epochs + 1)\n",
    "    \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses[:actual_epochs], 'g', label='Training loss')\n",
    "        plt.plot(epochs, val_losses[:actual_epochs], 'b', label='Validation loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_accuracies[:actual_epochs], 'g', label='Training accuracy')\n",
    "        plt.plot(epochs, val_accuracies[:actual_epochs], 'b', label='Validation accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        with h5py.File(file_name, 'w') as f:\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'weights'):\n",
    "                    f.create_dataset(f'{layer.name}_weights', data=layer.weights)\n",
    "                if hasattr(layer, 'biases'):\n",
    "                    f.create_dataset(f'{layer.name}_biases', data=layer.biases)\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        with h5py.File(file_name, 'r') as f:\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'weights'):\n",
    "                    layer.weights = f[f'{layer.name}_weights'][:]\n",
    "                if hasattr(layer, 'biases'):\n",
    "                    layer.biases = f[f'{layer.name}_biases'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462ca5d-03e7-406a-b2be-aa55e6a6e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training data shape: (80, 80, 3)\n",
      "\n",
      "\n",
      "  |====== Start training ======| \n",
      "\n",
      "Epoch: 0, Train Loss: 6.454484370271792, Train Accuracy: 0.10746812386156648, Val Loss: 7.455268345700294, Val Accuracy: 0.09174311926605505\n",
      "Epoch: 1, Train Loss: 8.19942247668675, Train Accuracy: 0.1092896174863388, Val Loss: 8.890938888288735, Val Accuracy: 0.13761467889908258\n",
      "Epoch: 2, Train Loss: 9.536742953794713, Train Accuracy: 0.10200364298724955, Val Loss: 11.241659193629511, Val Accuracy: 0.05504587155963303\n",
      "Epoch: 3, Train Loss: 10.788523235754202, Train Accuracy: 0.08014571948998178, Val Loss: 12.126639022480187, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 4, Train Loss: 11.16987148592099, Train Accuracy: 0.1092896174863388, Val Loss: 10.679597936916407, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 5, Train Loss: 11.412722330835258, Train Accuracy: 0.10200364298724955, Val Loss: 12.62048226381551, Val Accuracy: 0.11926605504587157\n",
      "Epoch: 7, Train Loss: 12.843473613685344, Train Accuracy: 0.08743169398907104, Val Loss: 13.337631999317246, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 8, Train Loss: 13.013472774303432, Train Accuracy: 0.11293260473588343, Val Loss: 13.22170750419575, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 9, Train Loss: 13.305870872083286, Train Accuracy: 0.10200364298724955, Val Loss: 13.481010248715977, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 10, Train Loss: 13.74245564700374, Train Accuracy: 0.10200364298724955, Val Loss: 14.404628749138976, Val Accuracy: 0.10091743119266056\n",
      "Epoch: 11, Train Loss: 13.617252586398573, Train Accuracy: 0.12386156648451731, Val Loss: 14.389777750208278, Val Accuracy: 0.10091743119266056\n"
     ]
    }
   ],
   "source": [
    "# Adjust the values to train\n",
    "num_classes = num_classes\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "threshold = 0.5\n",
    "early_stopping_patience = 15\n",
    "\n",
    "cnn = CNN(num_classes, epochs, learning_rate, threshold, early_stopping_patience)\n",
    "cnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9b5ef14-19d7-4b1b-91bc-e9e8953b057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = 'my_cnn_model'\n",
    "file_extension = 'h5'\n",
    "unique_filename = Limiting.get_unique_filename(base_filename, file_extension)\n",
    "\n",
    "# Save the model\n",
    "cnn.save_model(unique_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "408bbdf3-bd05-45c4-a520-68b1c4af972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model my_cnn_model_0.h5 loaded succesfully\n"
     ]
    }
   ],
   "source": [
    "# Later or in another script\n",
    "file_name = 'my_cnn_model_0.h5'\n",
    "if os.path.isfile(file_name):\n",
    "    cnn_loaded = CNN(num_classes=10)\n",
    "    cnn_loaded.load_model(file_name)\n",
    "    print(f\"Model {file_name} loaded succesfully\")\n",
    "else:\n",
    "    print(f\"The file {file_name} does not exist in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337c04-c9f5-413a-87b8-72d93bdfe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(test_images, predictions, true_labels):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(len(test_images)):\n",
    "        plt.subplot(1, len(test_images), i+1)\n",
    "        plt.imshow(test_images[i], cmap='gray')\n",
    "        plt.title(f\"Predicted: {predictions[i]}\\nTrue: {true_labels[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1391a90-2d8d-4350-bde5-d378a10f5bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8eae37-a2e7-43ba-ba2d-4e7d8d196535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
