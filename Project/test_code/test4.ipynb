{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4726704-d399-4d5e-9fdb-98c5ee1c8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a926fe9-d908-4fa1-9618-3bcab721e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 images\n",
      "Shape of the preprocessed images: (10, 150, 150, 3)\n",
      "Dimensions of the first preprocessed image: (150, 150, 3)\n",
      "Train set size: 7, Validation set size: 1, Test set size: 2\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        for i, img in enumerate(os.listdir(self.directory)):\n",
    "            if i >= 10:  # only load the first 10 images\n",
    "                break\n",
    "            try:\n",
    "                img_array = Image.open(os.path.join(self.directory, img))  # read the image\n",
    "                rgb_array = img_array.convert('RGB')  # ensure image is RGB\n",
    "                resized_array = rgb_array.resize((150, 150))  # resize the image\n",
    "                self.images.append(np.array(resized_array))\n",
    "                label = img.split('_')[0]  # extract label from filename\n",
    "                self.labels.append(label)\n",
    "                # print(f\"Loaded image {img} with label {label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "            \n",
    "    def preprocess_data(self):\n",
    "        self.images = np.array(self.images).reshape(-1, 150, 150, 3)\n",
    "        self.images = self.images / 255.0  # normalize pixel values\n",
    "        self.labels = np.array(self.labels)\n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "\n",
    "print(f\"Shape of the preprocessed images: {dataset.images.shape}\")\n",
    "# Print the dimensions of the first image\n",
    "print(f\"Dimensions of the first preprocessed image: {dataset.images[0].shape}\")\n",
    "\n",
    "# Get the images and labels\n",
    "images = dataset.images\n",
    "labels = dataset.labels\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c72535-3d41-446c-9137-426fc7d30723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Computing:\n",
    "    # ReLU activation function\n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # Derivative of ReLU\n",
    "    def relu_derivative(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    # Softmax function\n",
    "    def softmax(self, x):\n",
    "        e_x = [self.exp(i) for i in x]\n",
    "        return [i / sum(e_x) for i in e_x]\n",
    "\n",
    "    # Exponential function\n",
    "    def exp(self, x):\n",
    "        return 2.718281828459045 ** x  # approximation of e^x\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    def cross_entropy(self, predictions, targets):\n",
    "        return -sum([targets[i]*self.log(predictions[i]) for i in range(len(predictions))])\n",
    "\n",
    "    # Natural logarithm function\n",
    "    def log(self, x):\n",
    "        # Implement a method to compute the natural logarithm of x\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab5c62b9-8084-42a5-8d4b-13b51fb8ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[[[random.random() for _ in range(3)] for _ in range(filter_size)] for _ in range(filter_size)] for _ in range(num_filters)]\n",
    "        self.computing = Computing()\n",
    "\n",
    "    def zeros(shape):\n",
    "        if len(shape) == 1:\n",
    "            return [0 for _ in range(shape[0])]\n",
    "        elif len(shape) == 2:\n",
    "            return [[0 for _ in range(shape[1])] for _ in range(shape[0])]\n",
    "        elif len(shape) == 3:\n",
    "            return [[[0 for _ in range(shape[2])] for _ in range(shape[1])] for _ in range(shape[0])]\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, input):\n",
    "        h, w, num_filters = len(input), len(input[0]), self.num_filters\n",
    "        output = zeros((h - self.filter_size + 1, w - self.filter_size + 1, num_filters))\n",
    "        for i in range(h - self.filter_size + 1):\n",
    "            for j in range(w - self.filter_size + 1):\n",
    "                for f in range(num_filters):\n",
    "                    output[i][j][f] = dot(input[i:i+self.filter_size, j:j+self.filter_size], self.filters[f])\n",
    "        return output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = zeros(input.shape)\n",
    "        grad_filters = zeros(self.filters.shape)\n",
    "        for i in range(input.shape[0] - self.filter_size + 1):\n",
    "            for j in range(input.shape[1] - self.filter_size + 1):\n",
    "                for f in range(self.num_filters):\n",
    "                    grad_input[i:i+self.filter_size, j:j+self.filter_size] += grad_output[i, j, f] * self.filters[f]\n",
    "                    grad_filters[f] += grad_output[i, j, f] * input[i:i+self.filter_size, j:j+self.filter_size]\n",
    "        self.filters -= self.learning_rate * grad_filters\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class MaxPool2:\n",
    "    def __init__(self, filter_size):\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "\n",
    "        # Get the dimensions of the input\n",
    "        batch_size, h, w, num_filters = len(input), len(input[0]), len(input[0][0]), len(input[0][0][0])\n",
    "        output = [[[[0 for _ in range(num_filters)] for _ in range(w // self.filter_size)] for _ in range(h // self.filter_size)] for _ in range(batch_size)]\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(len(output[0])):\n",
    "                for j in range(len(output[0][0])):\n",
    "                    for f in range(num_filters):\n",
    "                        patch = []\n",
    "                        for di in range(self.filter_size):\n",
    "                            for dj in range(self.filter_size):\n",
    "                                patch.append(input[b][i* self.filter_size + di][j* self.filter_size + dj][f])\n",
    "                        output[b][i][j][f] = max(patch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.zeros(input.shape)\n",
    "\n",
    "        for i in range(input.shape[0] // self.filter_size):\n",
    "            for j in range(input.shape[1] // self.filter_size):\n",
    "                h_start = i * self.filter_size\n",
    "                h_end = (i + 1) * self.filter_size\n",
    "                w_start = j * self.filter_size\n",
    "                w_end = (j + 1) * self.filter_size\n",
    "\n",
    "                area_grad = grad_output[i, j]\n",
    "                max_val = np.amax(input[h_start:h_end, w_start:w_end], axis=(0, 1))\n",
    "                mask = input[h_start:h_end, w_start:w_end] == max_val\n",
    "                grad_input[h_start:h_end, w_start:w_end] = mask * area_grad\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    # Forward pass\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        return input.flatten()\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, input, grad_output):\n",
    "       return grad_output.reshape(self.last_input_shape)\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, input):\n",
    "        self.mask = (np.random.rand(*input.shape) > self.rate) / (1.0 - self.rate)\n",
    "        return input * self.mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, input, grad_output):\n",
    "        return grad_output * self.mask\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weights = [[0.01 for _ in range(output_units)] for _ in range(input_units)]\n",
    "        self.biases = [0 for _ in range(output_units)]\n",
    "        self.computing = Computing()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, input):\n",
    "        output = [0 for _ in range(len(self.biases))]\n",
    "        for i in range(len(input)):\n",
    "            for j in range(len(self.weights[0])):\n",
    "                output[j] += input[i] * self.weights[i][j]\n",
    "        for i in range(len(self.biases)):\n",
    "            output[i] += self.biases[i]\n",
    "        return output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        self.weights = self.weights - learning_rate * grad_weights\n",
    "        self.biases = self.biases - learning_rate * grad_biases\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.computing = Computing()\n",
    "\n",
    "    # Add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    # Compute loss\n",
    "    def compute_loss(self, predictions, labels):\n",
    "        return self.computing.cross_entropy(predictions, labels)\n",
    "\n",
    "    # Compute accuracy\n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        correct_predictions = np.sum(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1))\n",
    "        total_predictions = predictions.shape[0]\n",
    "        return correct_predictions / total_predictions\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, X, y):\n",
    "        # Compute the gradient of the loss\n",
    "        grad_output = self.compute_loss_grad(self.layers[-1].output, y)\n",
    "        # Propagate the gradient through the layers\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(layer.last_input, grad_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7c74d73-0ae6-4e95-86da-caaf8f6df68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cnn():\n",
    "    # Initialize the network\n",
    "    network = Network()\n",
    "\n",
    "    # Add layers to the network\n",
    "    network.add(ConvLayer(32, 3))  # Convolutional layer with 32 filters of size 3x3\n",
    "    network.add(MaxPool2(2))  # Max pooling layer with filter size 2x2\n",
    "    network.add(ConvLayer(64, 3))  # Another convolutional layer\n",
    "    network.add(MaxPool2(2))  # Another max pooling layer\n",
    "    network.add(ConvLayer(128, 3))  # Another convolutional layer\n",
    "    network.add(MaxPool2(2))  # Another max pooling layer\n",
    "    network.add(ConvLayer(256, 3))  # Another convolutional layer\n",
    "    network.add(MaxPool2(2))  # Another max pooling layer\n",
    "    network.add(Flatten())  # Flatten layer to prepare for fully connected layers\n",
    "    network.add(Dense(1024, 512))  # Fully connected layer with 1024 input units and 512 output units\n",
    "    network.add(Dense(512, 256))  # Fully connected layer with 512 input units and 256 output units\n",
    "    network.add(Dense(256, 128))  # Fully connected layer with 256 input units and 128 output units\n",
    "    network.add(Dense(128, 10))  # Output layer with 10 units (one for each class)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57797e2f-9848-4537-8302-267f43a8da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config(network, learning_rate, activator, loss, threshold, early_stopping):\n",
    "    # Set the network's activator, loss, and learning rate\n",
    "    network.activator = activator\n",
    "    network.loss = loss\n",
    "    network.learning_rate = learning_rate\n",
    "    network.threshold = threshold\n",
    "    network.early_stopping = early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb637fef-2335-4666-9c29-be335b4af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, x_train, y_train, epochs):\n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Perform a forward pass and compute the loss\n",
    "        predictions = network.forward(x_train)\n",
    "        loss = network.compute_loss(predictions, y_train)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        accuracy = network.compute_accuracy(predictions, y_train)\n",
    "\n",
    "        # Perform a backward pass and update the weights\n",
    "        network.backward(x_train, y_train)\n",
    "\n",
    "        # Print the loss and accuracy for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - loss: {loss}, accuracy: {accuracy}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if loss < best_loss - network.threshold:\n",
    "            best_loss = loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement == network.early_stopping:\n",
    "                print('Early stopping triggered')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da6bf889-e76a-41a9-9ce6-6feb7f3d2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Network object at 0x7efcacb93c50>\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "network = custom_cnn()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beef2312-374d-4dbb-a325-2a6dd6f48822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3977641\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"  # for all three GPUs\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # for the first GPU\n",
    "# import gpustat\n",
    "# print(gpustat.print_gpustat())\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fda021b-82ab-4e20-8300-ad0e08e201b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zeros' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m train_config(network, learning_rate, activator, loss, threshold, early_stopping)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# TOOOOOOOOOOOOOOOOOOOOO SLOOOOOOOOOOOWWWWWWWWWWWWWWWWWWWWWW\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, x_train, y_train, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Loop over the epochs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Perform a forward pass and compute the loss\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mcompute_loss(predictions, y_train)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Compute the accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 148\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 148\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m, in \u001b[0;36mConvLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     18\u001b[0m     h, w, num_filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters\n\u001b[0;32m---> 19\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m((h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, w \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_filters))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zeros' is not defined"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "epochs = 1\n",
    "learning_rate = 1\n",
    "activator = 'relu'\n",
    "loss = 'cross_entropy'\n",
    "threshold = 0.01\n",
    "early_stopping = 10\n",
    "\n",
    "# Configure the network for training\n",
    "train_config(network, learning_rate, activator, loss, threshold, early_stopping)\n",
    "\n",
    "# Train the network\n",
    "train(network, x_train, y_train, epochs)\n",
    "# TOOOOOOOOOOOOOOOOOOOOO SLOOOOOOOOOOOWWWWWWWWWWWWWWWWWWWWWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5a080-5457-433c-951d-c1e577b73944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99e011-43dc-43fa-8a2b-abf21bfb893f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1304a9-f24e-4735-9bb6-52c4c25dbb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
