{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 images\n",
      "Shape of the preprocessed images: 10 images, each with 150x150 pixels\n",
      "Dimensions of the first preprocessed image: 150x150 pixels\n",
      "Train set size: 7, Validation set size: 1, Test set size: 2\n",
      "Unique classes in the training set: {'hydrangeas', 'orchids', 'lilies', 'tulip', 'hibiscus', 'daisies', 'peonies'}\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        for i, img in enumerate(os.listdir(self.directory)):\n",
    "            if i >= 10:  # only load the first 10 images\n",
    "                break\n",
    "            try:\n",
    "                img_path = os.path.join(self.directory, img)\n",
    "                with Image.open(img_path) as img_array:  # read the image\n",
    "                    gray_array = img_array.convert('L')  # ensure image is grayscale\n",
    "                    resized_array = gray_array.resize((150, 150))  # resize the image\n",
    "                    self.images.append([list(resized_array.getdata())[i:i+150] for i in range(0, 22500, 150)])  # keep image in 2D\n",
    "                    label = img.split('_')[0]  # extract label from filename\n",
    "                    self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Normalize pixel values\n",
    "        self.images = [[[pixel / 255.0 for pixel in row] for row in image] for image in self.images]\n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "\n",
    "# Print the shape of the preprocessed images\n",
    "print(f\"Shape of the preprocessed images: {len(dataset.images)} images, each with {len(dataset.images[0])}x{len(dataset.images[0][0])} pixels\")\n",
    "\n",
    "# Dimensions of the first preprocessed image\n",
    "print(f\"Dimensions of the first preprocessed image: {len(dataset.images[0])}x{len(dataset.images[0][0])} pixels\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# Custom function to split the dataset\n",
    "def custom_train_test_split(data, labels, train_size):\n",
    "    train_count = int(len(data) * train_size)\n",
    "    return data[:train_count], data[train_count:], labels[:train_count], labels[train_count:]\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_temp, y_train, y_temp = custom_train_test_split(dataset.images, dataset.labels, train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set, validation is now 15%\n",
    "val_count = int(len(x_temp) * (validation_ratio / (test_ratio + validation_ratio)))\n",
    "x_val, x_test = x_temp[:val_count], x_temp[val_count:]\n",
    "y_val, y_test = y_temp[:val_count], y_temp[val_count:]\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = set(y_train)\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Computing:\n",
    "    def __init__(self, input_layer, output_layer):\n",
    "        self.input_layer = input_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.weights = [[random.random() for _ in range(output_layer)] for _ in range(input_layer)]\n",
    "        self.biases = [random.random() for _ in range(output_layer)]\n",
    "\n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = [math.exp(i - max(x)) for i in x]\n",
    "        return [i / sum(e_x) for i in e_x]\n",
    "\n",
    "    def cross_entropy(self, y_true, y_pred):\n",
    "        return -sum([y_true[i]*math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[[random.random() for _ in range(self.filter_size)] for _ in range(self.filter_size)] for _ in range(self.num_filters)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        h, w = len(input), len(input[0])\n",
    "        output = [[[0 for _ in range(self.num_filters)] for _ in range(w - self.filter_size + 1)] for _ in range(h - self.filter_size + 1)]\n",
    "        for i in range(h - self.filter_size + 1):\n",
    "            for j in range(w - self.filter_size + 1):\n",
    "                for f in range(self.num_filters):\n",
    "                    for i2 in range(self.filter_size):\n",
    "                        for j2 in range(self.filter_size):\n",
    "                            output[i][j][f] += input[i+i2][j+j2] * self.filters[f][i2][j2]\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_input = [[[0]*len(self.last_input[0][0]) for _ in range(len(self.last_input[0]))] for _ in range(len(self.last_input))]\n",
    "        for i in range(len(d_L_d_out)):\n",
    "            for j in range(len(d_L_d_out[0])):\n",
    "                for f in range(len(d_L_d_out[0][0])):\n",
    "                    for i2 in range(self.filter_size):\n",
    "                        for j2 in range(self.filter_size):\n",
    "                            for f2 in range(len(self.filters[0][0][0])):\n",
    "                                d_L_d_input[i+i2][j+j2][f2] += d_L_d_out[i][j][f] * self.filters[f][i2][j2][f2]\n",
    "        return d_L_d_input\n",
    "\n",
    "\n",
    "class MaxPooling:\n",
    "    def __init__(self, filter_size):\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        h, w, _ = len(input), len(input[0]), len(input[0][0])\n",
    "        \n",
    "        # Calculate the dimensions of the output\n",
    "        h_out = h // self.filter_size\n",
    "        w_out = w // self.filter_size\n",
    "        \n",
    "        # Initialize the output\n",
    "        output = [[[0]*_ for _ in range(w_out)] for _ in range(h_out)]\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                for f in range(_):\n",
    "                    # Flatten the 2D list into a 1D list before applying the max function\n",
    "                    flat_list = [input[i*self.filter_size+x][j*self.filter_size+y][f] for x in range(self.filter_size) for y in range(self.filter_size)]\n",
    "                    # Check if flat_list is a list of lists or a list of arrays\n",
    "                    if isinstance(flat_list[0], list) or isinstance(flat_list[0], np.ndarray):\n",
    "                        # Flatten the list of lists or list of arrays into a 1D list\n",
    "                        flat_list = [item for sublist in flat_list for item in sublist]\n",
    "                    output[i][j][f] = max(flat_list)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "        for i in range(0, self.last_input.shape[0], self.filter_size):\n",
    "            for j in range(0, self.last_input.shape[1], self.filter_size):\n",
    "                h, w, f = self.last_input[i:i+self.filter_size, j:j+self.filter_size].shape\n",
    "                max_val = np.amax(self.last_input[i:i+self.filter_size, j:j+self.filter_size], axis=(0, 1))\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            if self.last_input[i+i2, j+j2, f2] == max_val[f2]:\n",
    "                                d_L_d_input[i+i2, j+j2, f2] = d_L_d_out[i // self.filter_size, j // self.filter_size, f2]\n",
    "        return d_L_d_input\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_len, output_len):\n",
    "        self.weights = [[random.random() for _ in range(output_len)] for _ in range(input_len)]\n",
    "        self.biases = [random.random() for _ in range(output_len)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = (len(input), len(input[0]), len(input[0][0]))\n",
    "        input = [item for sublist in input for item in sublist]\n",
    "        input = [item for sublist in input for item in sublist]\n",
    "        self.last_input = input\n",
    "        input_len, output_len = len(self.weights), len(self.weights[0])\n",
    "        output = [0]*output_len\n",
    "        for i in range(input_len):\n",
    "            for j in range(output_len):\n",
    "                output[j] += self.weights[i][j] * input[i]\n",
    "        # Apply the activation function here\n",
    "        output = [self.relu(o + b) for o, b in zip(output, self.biases)]\n",
    "        return output\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = [math.exp(i - max(x)) for i in x]\n",
    "        return [i / sum(e_x) for i in e_x]\n",
    "    \n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_input = [0]*len(self.last_input)\n",
    "        for i, grad in enumerate(d_L_d_out):\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j][i] -= learn_rate * d_L_d_out[i] * self.last_input[j]\n",
    "                d_L_d_input[j] += self.weights[j][i] * d_L_d_out[i]\n",
    "        return d_L_d_input\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, drop_probability):\n",
    "        self.drop_probability = drop_probability\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        output = [0]*len(input)\n",
    "        self.dropped = []\n",
    "        for i in range(len(input)):\n",
    "            if random.random() < self.drop_probability:\n",
    "                output[i] = 0\n",
    "                self.dropped.append(True)\n",
    "            else:\n",
    "                output[i] = input[i]\n",
    "                self.dropped.append(False)\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        return [d_L_d_out[i] if not self.dropped[i] else 0 for i in range(len(d_L_d_out))]\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Cross-entropy loss\n",
    "        return -sum([y_true[i]*math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "    def loss_derivative(self, y_true, y_pred):\n",
    "        return [-y_true[i]/y_pred[i] for i in range(len(y_true))]\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        # Convert predictions to one-hot vectors\n",
    "        y_pred = [1 if p == max(y_pred) else 0 for p in y_pred]\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct_predictions = sum([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))])\n",
    "        accuracy = correct_predictions / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(X, grad)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dense):\n",
    "                for i in range(len(layer.weights)):\n",
    "                    for j in range(len(layer.weights[0])):\n",
    "                        layer.weights[i][j] -= learning_rate * layer.weights[i][j]\n",
    "                for i in range(len(layer.biases)):\n",
    "                    layer.biases[i] -= learning_rate * layer.biases[i]\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for e in range(epochs):\n",
    "            out = self.forward(X)\n",
    "            grad = self.loss_derivative(y, out)\n",
    "            self.backward(X, grad)\n",
    "            self.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd9fd6-5061-4411-aae1-39d1ba8e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Initialize the network\n",
    "    network = Network()\n",
    "\n",
    "    # Add layers to the network\n",
    "    # Adjust the parameters as needed\n",
    "    network.add_layer(Convolution(num_filters=32, filter_size=3))\n",
    "    network.add_layer(MaxPooling(filter_size=2))\n",
    "    network.add_layer(Convolution(num_filters=64, filter_size=3))\n",
    "    network.add_layer(MaxPooling(filter_size=2))\n",
    "    network.add_layer(Dense(input_len=64*37*37, output_len=128))  # Adjust input_len based on the output of the last MaxPooling layer\n",
    "    network.add_layer(Dropout(drop_probability=0.5))\n",
    "    network.add_layer(Dense(input_len=128, output_len=10))  # 10 output classes for 10 types of flowers\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59b39e-3a7e-4574-bf96-a0774654f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, epochs, learning_rate, threshold, early_stoppings):\n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    rounds_without_improvement = 0\n",
    "\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass and compute loss\n",
    "        output = model.forward(x_train)\n",
    "        loss = model.compute_loss(y_train, output)\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = model.compute_accuracy(y_train, output)\n",
    "\n",
    "        # Print loss and accuracy for this epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {loss} - Accuracy: {accuracy}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if loss < best_loss - threshold:\n",
    "            best_loss = loss\n",
    "            rounds_without_improvement = 0\n",
    "        else:\n",
    "            rounds_without_improvement += 1\n",
    "            if rounds_without_improvement == early_stopping:\n",
    "                print(\"Early stopping due to no improvement after\", early_stopping, \"rounds.\")\n",
    "                break\n",
    "\n",
    "        # Backward pass and update weights\n",
    "        grad = model.loss_derivative(y_train, output)\n",
    "        model.backward(x_train, grad)\n",
    "        model.update(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fffbd5-21fa-410f-bf6e-37bd61f1b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Define your training parameters\n",
    "epochs = 10  # Number of epochs to train for\n",
    "learning_rate = 1  # Learning rate\n",
    "threshold = 0.01  # Threshold for early stopping\n",
    "early_stopping = 10  # Number of rounds without improvement before early stopping\n",
    "\n",
    "# Train the model\n",
    "train_model(model, x_train, y_train, epochs, learning_rate, threshold, early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
