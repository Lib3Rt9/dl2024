{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 images\n",
      "Shape of the preprocessed images: (10, 150, 150, 1)\n",
      "Dimensions of the first preprocessed image: (150, 150, 1)\n",
      "Train set size: 7, Validation set size: 1, Test set size: 2\n",
      "Unique classes in the training set: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        for i, img in enumerate(os.listdir(self.directory)):\n",
    "            if i >= 10:  # only load the first 10 images\n",
    "                break\n",
    "            try:\n",
    "                img_array = Image.open(os.path.join(self.directory, img))  # read the image\n",
    "                gray_array = img_array.convert('L')  # ensure image is grayscale\n",
    "                resized_array = gray_array.resize((150, 150))  # resize the image\n",
    "                self.images.append(np.array(resized_array))\n",
    "                label = img.split('_')[0]  # extract label from filename\n",
    "                self.labels.append(label)\n",
    "                # print(f\"Loaded image {img} with label {label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.images = np.array(self.images).reshape(-1, 150, 150, 1)  # update the reshape dimensions for grayscale\n",
    "        self.images = self.images / 255.0  # normalize pixel values\n",
    "        self.labels = np.array(self.labels)\n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "\n",
    "print(f\"Shape of the preprocessed images: {dataset.images.shape}\")\n",
    "print(f\"Dimensions of the first preprocessed image: {dataset.images[0].shape}\") # dimensions of first image\n",
    "\n",
    "# Get the images and labels\n",
    "images = dataset.images\n",
    "labels = dataset.labels\n",
    "\n",
    "# Convert labels to numerical values\n",
    "unique_labels = list(set(labels))\n",
    "labels_encoded = [unique_labels.index(l) for l in labels]\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels_encoded_one_hot = one_hot_encode(labels_encoded)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels_encoded_one_hot, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Convert one-hot encoded labels back to class indices\n",
    "y_train_indices = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = set(y_train_indices)\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    if isinstance(x, list):\n",
    "        return [max(0, xi) for xi in x]\n",
    "    else:\n",
    "        return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if isinstance(x, list):\n",
    "        return [1 if xi > 0 else 0 for xi in x]\n",
    "    else:\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def gradient_mse(y_true, y_pred):\n",
    "    return [2 * (yp - yt) for yt, yp in zip(y_true, y_pred)]\n",
    "\n",
    "def softmax(x):\n",
    "    # Check if x is a list of lists\n",
    "    if isinstance(x[0], list):\n",
    "        # Apply softmax to each list of logits in x\n",
    "        return [softmax(xi) for xi in x]\n",
    "    else:\n",
    "        # Apply softmax to a single list of logits\n",
    "        max_x = max(x)\n",
    "        exps = [math.exp(i - max_x) for i in x]\n",
    "        sum_of_exps = sum(exps)\n",
    "        return [j / sum_of_exps for j in exps]\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    # Assuming predictions is a list of probabilities and targets is a list of one-hot encoded classes\n",
    "    N = len(predictions)\n",
    "    ce = 0\n",
    "    for p, t in zip(predictions, targets):\n",
    "        ce -= t * math.log(p) if p > 0 else 0  # Adding check to prevent math domain error\n",
    "    ce /= N\n",
    "    return ce\n",
    "     \n",
    "# Layer classes\n",
    "class Layer:\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[random.random() for _ in range(filter_size * filter_size)] for _ in range(num_filters)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        height, width = len(input), len(input[0])\n",
    "        output_height = height - self.filter_size + 1\n",
    "        output_width = width - self.filter_size + 1\n",
    "        output = [[0 for _ in range(output_width)] for _ in range(output_height)]\n",
    "\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                for f in range(self.num_filters):\n",
    "                    output[i][j] += sum(\n",
    "                        input[i + m][j + n] * self.filters[f][m * self.filter_size + n]\n",
    "                        for m in range(self.filter_size)\n",
    "                        for n in range(self.filter_size)\n",
    "                    )\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        d_filters = [[[0 for _ in range(self.filter_size * self.filter_size)] for _ in range(self.num_filters)]]\n",
    "        for i in range(len(input) - self.filter_size + 1):\n",
    "            for j in range(len(input[0]) - self.filter_size + 1):\n",
    "                for f in range(self.num_filters):\n",
    "                    for m in range(self.filter_size):\n",
    "                        for n in range(self.filter_size):\n",
    "                            d_filters[f][m * self.filter_size + n] += input[i + m][j + n] * gradient[i][j]\n",
    "        return d_filters\n",
    "\n",
    "class MaxPooling2D(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        height, width = len(input), len(input[0])\n",
    "        new_height = height // self.pool_size\n",
    "        new_width = width // self.pool_size\n",
    "        output = [[0 for _ in range(new_width)] for _ in range(new_height)]\n",
    "\n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                output[i][j] = max(\n",
    "                    input[i * self.pool_size + m][j * self.pool_size + n]\n",
    "                    for m in range(self.pool_size)\n",
    "                    for n in range(self.pool_size)\n",
    "                )\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        d_input = [[0 for _ in range(len(input[0]))] for _ in range(len(input))]\n",
    "        for i in range(0, len(input), self.pool_size):\n",
    "            for j in range(0, len(input[0]), self.pool_size):\n",
    "                window = [input[i + m][j + n] for m in range(self.pool_size) for n in range(self.pool_size)]\n",
    "                max_value = max(window)\n",
    "                for m in range(self.pool_size):\n",
    "                    for n in range(self.pool_size):\n",
    "                        if input[i + m][j + n] == max_value:\n",
    "                            d_input[i + m][j + n] = gradient[i // self.pool_size][j // self.pool_size]\n",
    "        return d_input\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input_shape = (len(input), len(input[0]))  # save this for backward pass\n",
    "        return [item for sublist in input for item in sublist]  # flatten the list\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        return [gradient[i:i+self.input_shape[1]] for i in range(0, len(gradient), self.input_shape[1])]\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = [[random.random() for _ in range(num_inputs)] for _ in range(num_outputs)]\n",
    "        self.biases = [random.random() for _ in range(num_outputs)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Compute the weighted sum of inputs plus biases\n",
    "        return [[sum(i * w + b for i, w in zip(input_sample, weights)) + b for weights, b in zip(self.weights, self.biases)] for input_sample in input]\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        # Compute gradients with respect to weights and biases\n",
    "        d_weights = [[i * g for i in input] for g in gradient]\n",
    "        d_biases = gradient\n",
    "        d_input = [0 for _ in range(len(input))]\n",
    "        for i, g in enumerate(gradient):\n",
    "            for j, w in enumerate(self.weights[i]):\n",
    "                d_input[j] += g * w\n",
    "        return d_input, d_weights, d_biases\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = (len(input), len(input[0]))  # save this for backward pass\n",
    "        self.mask = [[random.random() > self.rate for _ in range(self.input_shape[1])] for _ in range(self.input_shape[0])]\n",
    "        return [[input[i][j] * self.mask[i][j] for j in range(self.input_shape[1])] for i in range(self.input_shape[0])]\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        return [[gradient[i][j] * self.mask[i][j] for j in range(self.input_shape[1])] for i in range(self.input_shape[0])]\n",
    "\n",
    "# Activation functions as layers\n",
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        # Apply ReLU activation element-wise\n",
    "        return [relu(x) for x in input]\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        # Apply the derivative of ReLU to the gradient\n",
    "        return [g * relu_derivative(x) for x, g in zip(input, gradient)]\n",
    "\n",
    "# Loss function as a class\n",
    "class SoftmaxCrossEntropyLoss:\n",
    "    def forward(self, logits, labels):\n",
    "        self.predictions = softmax(logits)\n",
    "        return cross_entropy(self.predictions, labels)\n",
    "\n",
    "    def backward(self, logits, labels):\n",
    "        return [p - l for p, l in zip(self.predictions, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10eacd7-cf29-4d54-8573-a11137dd7188",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomCNNModel()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mCustomCNNModel.train\u001b[0;34m(self, x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping)\u001b[0m\n\u001b[1;32m     40\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_train, y_train):\n\u001b[0;32m---> 42\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(logits, y)\n\u001b[1;32m     44\u001b[0m     train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mCustomCNNModel.forward\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 165\u001b[0m, in \u001b[0;36mSoftmaxCrossEntropyLoss.forward\u001b[0;34m(self, logits, labels)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, labels):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     34\u001b[0m ce \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, targets):\n\u001b[0;32m---> 36\u001b[0m     ce \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(p) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Adding check to prevent math domain error\u001b[39;00m\n\u001b[1;32m     37\u001b[0m ce \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ce\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "class CustomCNNModel:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Conv2D(32, 3),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(2),\n",
    "            Conv2D(64, 3),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(2),\n",
    "            Conv2D(128, 3),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(2),\n",
    "            Flatten(),\n",
    "            Dense(128, 128),\n",
    "            ReLU(),\n",
    "            Dropout(0.1),\n",
    "            Dense(128, 3)\n",
    "        ]\n",
    "        self.loss = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        self.last_input_shape = input.shape\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return self.loss.forward(input, labels)\n",
    "\n",
    "    def backward(self, logits, labels):\n",
    "        gradient = self.loss.backward(logits, labels)\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping):\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        best_val_loss = float('inf')\n",
    "        no_improvement = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                logits = self.forward(x, y)\n",
    "                train_loss += self.loss.forward(logits, y)\n",
    "                train_correct += np.argmax(logits) == np.argmax(y)\n",
    "                gradient = self.backward(logits, y)\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, (Conv2D, Dense)):\n",
    "                        layer.weights -= learning_rate * gradient\n",
    "\n",
    "            train_loss /= len(x_train)\n",
    "            train_acc = train_correct / len(x_train)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            for x, y in zip(x_val, y_val):\n",
    "                logits = self.forward(x, y)\n",
    "                val_loss += self.loss.forward(logits, y)\n",
    "                val_correct += np.argmax(logits) == np.argmax(y)\n",
    "\n",
    "            val_loss /= len(x_val)\n",
    "            val_acc = val_correct / len(x_val)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss - threshold:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Stopping training after {epoch+1} epochs due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "# Define your training parameters\n",
    "epochs = 10  # Number of epochs to train for\n",
    "learning_rate = 1  # Learning rate\n",
    "threshold = 0.01  # Threshold for early stopping\n",
    "early_stopping = 10  # Number of rounds without improvement before early stopping\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomCNNModel()\n",
    "\n",
    "# Train the model\n",
    "history = model.train(x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
