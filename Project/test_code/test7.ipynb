{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2be59-1599-488c-a07e-be32e73da290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # to process the data\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3e9f9f-ff9b-40a2-bf06-8894b36a5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 images\n",
      "Shape of the preprocessed images: 10 images, each with 150x150 pixels\n",
      "Dimensions of the first preprocessed image: 150x150 pixels\n",
      "Train set size: 7, Validation set size: 1, Test set size: 2\n",
      "Unique classes in the training set: {'orchids', 'hibiscus', 'daisies', 'hydrangeas', 'lilies', 'peonies', 'tulip'}\n"
     ]
    }
   ],
   "source": [
    "class FlowerDataset:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        for i, img in enumerate(os.listdir(self.directory)):\n",
    "            if i >= 10:  # only load the first 10 images\n",
    "                break\n",
    "            try:\n",
    "                img_path = os.path.join(self.directory, img)\n",
    "                with Image.open(img_path) as img_array:  # read the image\n",
    "                    gray_array = img_array.convert('L')  # ensure image is grayscale\n",
    "                    resized_array = gray_array.resize((150, 150))  # resize the image\n",
    "                    self.images.append([list(resized_array.getdata())[i:i+150] for i in range(0, 22500, 150)])  # keep image in 2D\n",
    "                    label = img.split('_')[0]  # extract label from filename\n",
    "                    self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Normalize pixel values\n",
    "        self.images = [[[pixel / 255.0 for pixel in row] for row in image] for image in self.images]\n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "dataset = FlowerDataset('flowers')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "\n",
    "# Print the shape of the preprocessed images\n",
    "print(f\"Shape of the preprocessed images: {len(dataset.images)} images, each with {len(dataset.images[0])}x{len(dataset.images[0][0])} pixels\")\n",
    "\n",
    "# Dimensions of the first preprocessed image\n",
    "print(f\"Dimensions of the first preprocessed image: {len(dataset.images[0])}x{len(dataset.images[0][0])} pixels\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# Custom function to split the dataset\n",
    "def custom_train_test_split(data, labels, train_size):\n",
    "    train_count = int(len(data) * train_size)\n",
    "    return data[:train_count], data[train_count:], labels[:train_count], labels[train_count:]\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_temp, y_train, y_temp = custom_train_test_split(dataset.images, dataset.labels, train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set, validation is now 15%\n",
    "val_count = int(len(x_temp) * (validation_ratio / (test_ratio + validation_ratio)))\n",
    "x_val, x_test = x_temp[:val_count], x_temp[val_count:]\n",
    "y_val, y_test = y_temp[:val_count], y_temp[val_count:]\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Train set size: {len(x_train)}, Validation set size: {len(x_val)}, Test set size: {len(x_test)}\")\n",
    "\n",
    "# Print the unique classes in the training set\n",
    "unique_classes = set(y_train)\n",
    "print(f\"Unique classes in the training set: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f818a85a-2f0e-42d1-8201-60703ddfb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    if isinstance(x, list):\n",
    "        return [max(0, xi) for xi in x]\n",
    "    else:\n",
    "        return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if isinstance(x, list):\n",
    "        return [1 if xi > 0 else 0 for xi in x]\n",
    "    else:\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def gradient_mse(y_true, y_pred):\n",
    "    return [2 * (yp - yt) for yt, yp in zip(y_true, y_pred)]\n",
    "\n",
    "def softmax(x):\n",
    "    exps = [math.exp(i) for i in x]\n",
    "    sum_of_exps = sum(exps)\n",
    "    return [j / sum_of_exps for j in exps]\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    # Assuming predictions is a list of probabilities and targets is a list of one-hot encoded classes\n",
    "    N = len(predictions)\n",
    "    ce = 0\n",
    "    for p, t in zip(predictions, targets):\n",
    "        ce -= t * math.log(p) if p > 0 else 0  # Adding check to prevent math domain error\n",
    "    ce /= N\n",
    "    return ce\n",
    "\n",
    "def reshape_input(flat_input, image_height, image_width):\n",
    "    return [flat_input[i * image_width:(i + 1) * image_width] for i in range(image_height)]\n",
    "\n",
    "# Layer classes\n",
    "class Layer:\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = [[random.random() for _ in range(filter_size * filter_size)] for _ in range(num_filters)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assume input is a 2D list representing the image\n",
    "        height, width = len(input), len(input[0])\n",
    "        output_height = height - self.filter_size + 1\n",
    "        output_width = width - self.filter_size + 1\n",
    "        # Initialize output with zeros\n",
    "        output = [[0 for _ in range(output_width)] for _ in range(output_height)]\n",
    "\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                for f in range(self.num_filters):\n",
    "                    filter_output = 0\n",
    "                    for m in range(self.filter_size):\n",
    "                        for n in range(self.filter_size):\n",
    "                            filter_output += input[i + m][j + n] * self.filters[f][m * self.filter_size + n]\n",
    "                    output[i][j] = filter_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        d_filters = [[[0 for _ in range(self.filter_size * self.filter_size)] for _ in range(self.num_filters)]]\n",
    "        for i in range(len(input) - self.filter_size + 1):\n",
    "            for j in range(len(input[0]) - self.filter_size + 1):\n",
    "                for f in range(self.num_filters):\n",
    "                    for m in range(self.filter_size):\n",
    "                        for n in range(self.filter_size):\n",
    "                            d_filters[f][m * self.filter_size + n] += input[i + m][j + n] * gradient[i][j]\n",
    "        return d_filters\n",
    "\n",
    "class MaxPooling2D(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assume input is a 2D list representing the image\n",
    "        height, width = len(input), len(input[0])\n",
    "        new_height = height // self.pool_size\n",
    "        new_width = width // self.pool_size\n",
    "        # Initialize output with zeros\n",
    "        output = [[0 for _ in range(new_width)] for _ in range(new_height)]\n",
    "\n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                pool = [input[i * self.pool_size + m][j * self.pool_size + n]\n",
    "                        for m in range(self.pool_size)\n",
    "                        for n in range(self.pool_size)]\n",
    "                output[i][j] = max(pool)\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        d_input = [[0 for _ in range(len(input[0]))] for _ in range(len(input))]\n",
    "        for i in range(0, len(input), self.pool_size):\n",
    "            for j in range(0, len(input[0]), self.pool_size):\n",
    "                window = [input[i + m][j + n] for m in range(self.pool_size) for n in range(self.pool_size)]\n",
    "                max_value = max(window)\n",
    "                for m in range(self.pool_size):\n",
    "                    for n in range(self.pool_size):\n",
    "                        if input[i + m][j + n] == max_value:\n",
    "                            d_input[i + m][j + n] = gradient[i // self.pool_size][j // self.pool_size]\n",
    "        return d_input\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input_shape = (len(input), len(input[0]))  # save this for backward pass\n",
    "        return [item for sublist in input for item in sublist]  # flatten the list\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        return [gradient[i:i+self.input_shape[1]] for i in range(0, len(gradient), self.input_shape[1])]\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = [[random.random() for _ in range(num_inputs)] for _ in range(num_outputs)]\n",
    "        self.biases = [random.random() for _ in range(num_outputs)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Compute the weighted sum of inputs plus biases\n",
    "        return [sum(i * w + b for i, w in zip(input, weights_row)) for weights_row, b in zip(self.weights, self.biases)]\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        # Compute gradients with respect to weights and biases\n",
    "        d_weights = [[i * g for i in input] for g in gradient]\n",
    "        d_biases = gradient\n",
    "        d_input = [0 for _ in range(len(input))]\n",
    "        for i, g in enumerate(gradient):\n",
    "            for j, w in enumerate(self.weights[i]):\n",
    "                d_input[j] += g * w\n",
    "        return d_input, d_weights, d_biases\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = (len(input), len(input[0]))  # save this for backward pass\n",
    "        self.mask = [[random.random() > self.rate for _ in range(self.input_shape[1])] for _ in range(self.input_shape[0])]\n",
    "        return [[input[i][j] * self.mask[i][j] for j in range(self.input_shape[1])] for i in range(self.input_shape[0])]\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        return [[gradient[i][j] * self.mask[i][j] for j in range(self.input_shape[1])] for i in range(self.input_shape[0])]\n",
    "\n",
    "# Activation functions as layers\n",
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        output = [relu(x) for x in input]\n",
    "        # print(f\"ReLU output shape: {len(output)}x{len(output[0])}, type: {type(output)}\")  # Add this line\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, gradient):\n",
    "        # Apply the derivative of ReLU to the gradient\n",
    "        return [g * relu_derivative(x) for x, g in zip(input, gradient)]\n",
    "\n",
    "# Loss function as a class\n",
    "class SoftmaxCrossEntropyLoss:\n",
    "    def forward(self, logits, labels):\n",
    "        self.predictions = softmax(logits)\n",
    "        return cross_entropy(self.predictions, labels)\n",
    "\n",
    "    def backward(self, logits, labels):\n",
    "        return [p - l for p, l in zip(self.predictions, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b10eacd7-cf29-4d54-8573-a11137dd7188",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomCNNModel()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mCustomCNNModel.train\u001b[0;34m(self, x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping)\u001b[0m\n\u001b[1;32m     28\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_train, y_train):\n\u001b[0;32m---> 30\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Apply softmax to the logits to get probabilities\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m softmax(logits)\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mCustomCNNModel.forward\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Apply softmax to the logits to get probabilities\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(probabilities, labels)\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x):\n\u001b[0;32m---> 20\u001b[0m     exps \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m     sum_of_exps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(exps)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [j \u001b[38;5;241m/\u001b[39m sum_of_exps \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m exps]\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x):\n\u001b[0;32m---> 20\u001b[0m     exps \u001b[38;5;241m=\u001b[39m [\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m     21\u001b[0m     sum_of_exps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(exps)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [j \u001b[38;5;241m/\u001b[39m sum_of_exps \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m exps]\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not list"
     ]
    }
   ],
   "source": [
    "class CustomCNNModel:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            \n",
    "        ]\n",
    "        self.loss = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        # Apply softmax to the logits to get probabilities\n",
    "        probabilities = softmax(input)\n",
    "        return self.loss.forward(probabilities, labels)\n",
    "\n",
    "    def backward(self, logits, labels):\n",
    "        gradient = self.loss.backward(logits, labels)\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping):\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        best_val_loss = float('inf')\n",
    "        no_improvement = 0\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                logits = self.forward(x, y)\n",
    "                # Apply softmax to the logits to get probabilities\n",
    "                probabilities = softmax(logits)\n",
    "                train_loss += self.loss.forward(probabilities, y)\n",
    "                train_correct += self.argmax(logits) == self.argmax(y)\n",
    "                gradient = self.loss.backward(logits, y)\n",
    "                for layer in reversed(self.layers):\n",
    "                    if isinstance(layer, Dense):\n",
    "                        gradient, d_weights, d_biases = layer.backward(x, gradient)\n",
    "                        # Update weights and biases\n",
    "                        layer.weights = [[w - learning_rate * dw for w, dw in zip(weights_row, d_weights_row)] for weights_row, d_weights_row in zip(layer.weights, d_weights)]\n",
    "                        layer.biases = [b - learning_rate * db for b, db in zip(layer.biases, d_biases)]\n",
    "                    else:\n",
    "                        gradient = layer.backward(x, gradient)\n",
    "                    \n",
    "            train_loss /= len(x_train)\n",
    "            train_acc = train_correct / len(x_train)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            for x, y in zip(x_val, y_val):\n",
    "                logits = self.forward(x, y)\n",
    "                val_loss += self.loss.forward(logits, y)\n",
    "                val_correct += self.argmax(logits) == self.argmax(y)\n",
    "\n",
    "            val_loss /= len(x_val)\n",
    "            val_acc = val_correct / len(x_val)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            # Print training and validation results\n",
    "            # Print loss and accuracy for this epoch\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss - threshold:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Stopping training after {epoch+1} epochs due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "        return history\n",
    "\n",
    "    def argmax(self, logits):\n",
    "        # Implement argmax without NumPy\n",
    "        return max(range(len(logits)), key=lambda i: logits[i])\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the model\n",
    "flat_input = [0.0] * 22500  # Example flat input list\n",
    "image_height = 150  # The height of the image\n",
    "image_width = 150  # The width of the image\n",
    "input_2d = reshape_input(flat_input, image_height, image_width)\n",
    "\n",
    "# Define your training parameters\n",
    "epochs = 10  # Number of epochs to train for\n",
    "learning_rate = 1  # Learning rate\n",
    "threshold = 0.01  # Threshold for early stopping\n",
    "early_stopping = 10  # Number of rounds without improvement before early stopping\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomCNNModel()\n",
    "\n",
    "# Train the model\n",
    "history = model.train(x_train, y_train, x_val, y_val, epochs, learning_rate, threshold, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926a82-5ea1-4872-9689-f4c36ac62c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
